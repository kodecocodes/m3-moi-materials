{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6d8f24-71f5-42a4-a85e-48e210d1e723",
   "metadata": {},
   "source": [
    "# Lesson 2 Project: Image Analysis with GPT-4 Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59acf5b1-8172-4398-ab9a-a5d393e47518",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb609c-d40f-4474-9d96-1c6e591191f3",
   "metadata": {},
   "source": [
    "Welcome to the second lesson on multimodal AI! Today, you're diving into the fascinating world of image analysis using GPT-4 Vision. Imagine an AI system that can look at an image and describe it in detail, and answer questions about its content. In this lesson, you'll explore the fundamentals of how GPT-4 Vision works, its applications, and its current limitations. You'll gain hands-on experience in using this technology, learning how to prepare images for analysis, make API requests, and interpret the API responses.\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Implement image analysis using GPT-4 with vision capabilities\n",
    "- Process and prepare images for API requests\n",
    "- Interpret and utilize the AI's analysis of image content\n",
    "\n",
    "These skills will not only give you a deeper understanding of multimodal AI but also equip you with practical knowledge that's highly relevant in your industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a7f32-243d-4e24-9f65-66de03b7e06a",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befd442-f09a-4744-afca-5931aab6c58e",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2ad69-59d6-447c-9d79-41a776a8794a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65cbe57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the OpenAI library\n",
    "# Set up relevant environment variables\n",
    "# Create the OpenAI connection object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb09b4c-3e42-47b6-9e14-18036e4eeac6",
   "metadata": {},
   "source": [
    "## Making GPT-4 Vision API Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2ab44-5dda-4f70-8131-08c5af305eb1",
   "metadata": {},
   "source": [
    "### Using Image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9d693",
   "metadata": {},
   "source": [
    "To ensure the image is suitable for analysis, it is recommended to download and view the image locally before passing it to GPT-4 Vision. You can use the following code to download an image from a URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193813b-1a99-4ebe-bd5f-eb14892e493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images in Jupyter Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382a097-bede-4aea-b69d-013463bd17a1",
   "metadata": {},
   "source": [
    "Now, lets make an API request to GPT-4 Vision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e8d2b-8e81-4ca9-924f-fc57c983313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an image URL when analyzing an image with GPT-4 Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34c8eb-22f9-4466-bf4d-9ff51ab0990f",
   "metadata": {},
   "source": [
    "### Uploading Base64 Encoded Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947e656-e23d-4ed9-a5d8-426a915c795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an image to a base64 encoded image\n",
    "# Upload the base64 encoded image to the OpenAI API server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1d097-10b1-4918-b38c-0784f698eb36",
   "metadata": {},
   "source": [
    "## Using More than One Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd08ab2-5c02-46db-a3fd-fc44c0fda452",
   "metadata": {},
   "source": [
    "You can upload multiple images to OpenAI and send a text query about those images. For example, you can compare food found in those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e86ea-380b-42f7-b80d-84b0dd38f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an API request consisting of two images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655a699-732f-46d5-9cb3-89a78ccbc86f",
   "metadata": {},
   "source": [
    "### Controlling Image Fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8aa3c-5a4b-4abf-9946-ab0dacb9d223",
   "metadata": {},
   "source": [
    "GPT-4 Vision allows you to control the fidelity of image processing using the detail parameter. There are three options:\n",
    "\n",
    "- \"low\": Faster processing, lower detail\n",
    "- \"high\": Slower processing, higher detail\n",
    "- \"auto\": The model decides based on the image size\n",
    "\n",
    "Here's how to use the detail parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c174d9d-345d-473c-8a8e-ed317dff232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the detail parameter when analyzing an image with GPT-4 Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc35b0a-d434-48bf-8d70-b2f9dec245c8",
   "metadata": {},
   "source": [
    "## Interpreting and Utilizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0f342-adcc-4317-b756-e531658985ef",
   "metadata": {},
   "source": [
    "When interpreting the results from GPT-4 Vision, keep in mind its capabilities and limitations:\n",
    "\n",
    "- The model excels at general descriptions and identifying objects in images.\n",
    "- It can understand relationships between objects but may struggle with precise spatial reasoning.\n",
    "- The model may provide approximate counts for objects in images.\n",
    "- It may have difficulty with very small text or highly specialized images (e.g., medical scans).\n",
    "\n",
    "Here's an example of how to extract specific information from the model's response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ea1a2-43b8-46d4-8b5d-6aeae5b27905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting specific information when analyzing an image from GPT-4 Vision\n",
    "# Use JSON format to make extracting information easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d5577-88b5-429f-a72a-67bd0409edf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

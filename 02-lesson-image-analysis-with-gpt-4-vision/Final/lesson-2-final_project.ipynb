{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6d8f24-71f5-42a4-a85e-48e210d1e723",
   "metadata": {},
   "source": [
    "# Lesson 2 Project: Image Analysis with GPT-4 Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59acf5b1-8172-4398-ab9a-a5d393e47518",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb609c-d40f-4474-9d96-1c6e591191f3",
   "metadata": {},
   "source": [
    "Welcome to the second lesson on multimodal AI! Today, you're diving into the fascinating world of image analysis using GPT-4 Vision. Imagine an AI system that can look at an image and describe it in detail, and answer questions about its content. In this lesson, you'll explore the fundamentals of how GPT-4 Vision works, its applications, and its current limitations. You'll gain hands-on experience in using this technology, learning how to prepare images for analysis, make API requests, and interpret the API responses.\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Implement image analysis using GPT-4 with vision capabilities\n",
    "- Process and prepare images for API requests\n",
    "- Interpret and utilize the AI's analysis of image content\n",
    "\n",
    "These skills will not only give you a deeper understanding of multimodal AI but also equip you with practical knowledge that's highly relevant in your industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a7f32-243d-4e24-9f65-66de03b7e06a",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befd442-f09a-4744-afca-5931aab6c58e",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e2ad69-59d6-447c-9d79-41a776a8794a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install openai pydantic python-dotenv Pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65cbe57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up relevant environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the OpenAI connection object\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb09b4c-3e42-47b6-9e14-18036e4eeac6",
   "metadata": {},
   "source": [
    "## Making GPT-4 Vision API Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2ab44-5dda-4f70-8131-08c5af305eb1",
   "metadata": {},
   "source": [
    "### Using Image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9d693",
   "metadata": {},
   "source": [
    "To ensure the image is suitable for analysis, it is recommended to download and view the image locally before passing it to GPT-4 Vision. You can use the following code to download an image from a URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6193813b-1a99-4ebe-bd5f-eb14892e493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images in Jupyter Lab\n",
    "\n",
    "# Import necessary libraries\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set image URL\n",
    "ramen_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Shoyu_ramen%2C_at_Kasukabe_Station_%282014.05.05%29_1.jpg/1280px-Shoyu_ramen%2C_at_Kasukabe_Station_%282014.05.05%29_1.jpg\"\n",
    "\n",
    "# Fetch the image from the URL\n",
    "headers = {\n",
    "    'User-Agent': 'User Agent 1.0'\n",
    "}\n",
    "response = requests.get(ramen_image_url, headers=headers, stream=True)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382a097-bede-4aea-b69d-013463bd17a1",
   "metadata": {},
   "source": [
    "Now, lets make an API request to GPT-4 Vision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334e8d2b-8e81-4ca9-924f-fc57c983313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an image URL when analyzing an image with GPT-4 Vision\n",
    "\n",
    "# Text prompt\n",
    "prompt = \"How much calories are in this food?\"\n",
    "\n",
    "# Model\n",
    "openai_model = \"gpt-4o\"\n",
    "\n",
    "# Creating an API request\n",
    "response = client.chat.completions.create(\n",
    "  model=openai_model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": ramen_image_url,\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "choice = response.choices[0]\n",
    "print(choice)\n",
    "\n",
    "# Extract the content\n",
    "print(choice.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34c8eb-22f9-4466-bf4d-9ff51ab0990f",
   "metadata": {},
   "source": [
    "### Uploading Base64 Encoded Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e947e656-e23d-4ed9-a5d8-426a915c795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an image to a base64 encoded image\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_path = \"images/fried_rice.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "# Show the image in Jupyter Lab\n",
    "img = Image.open(image_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Upload the base64 encoded image to the OpenAI API server\n",
    "\n",
    "# Text prompt\n",
    "prompt = \"How much calories are in this food?\"\n",
    "\n",
    "# Model\n",
    "openai_model = \"gpt-4o\"\n",
    "\n",
    "# Creating an API request\n",
    "response = client.chat.completions.create(\n",
    "  model=openai_model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "choice = response.choices[0]\n",
    "print(choice)\n",
    "\n",
    "# Extract the content\n",
    "print(choice.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1d097-10b1-4918-b38c-0784f698eb36",
   "metadata": {},
   "source": [
    "## Using More than One Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd08ab2-5c02-46db-a3fd-fc44c0fda452",
   "metadata": {},
   "source": [
    "You can upload multiple images to OpenAI and send a text query about those images. For example, you can compare food found in those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17e86ea-380b-42f7-b80d-84b0dd38f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an API request consisting of two images\n",
    "\n",
    "# Text prompt\n",
    "prompt = \"Which food has less calories?\"\n",
    "\n",
    "# Model\n",
    "openai_model = \"gpt-4o\"\n",
    "\n",
    "# Creating an API request\n",
    "response = client.chat.completions.create(\n",
    "  model=openai_model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "          },\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": ramen_image_url,\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "choice = response.choices[0]\n",
    "print(choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655a699-732f-46d5-9cb3-89a78ccbc86f",
   "metadata": {},
   "source": [
    "### Controlling Image Fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8aa3c-5a4b-4abf-9946-ab0dacb9d223",
   "metadata": {},
   "source": [
    "GPT-4 Vision allows you to control the fidelity of image processing using the detail parameter. There are three options:\n",
    "\n",
    "- \"low\": Faster processing, lower detail\n",
    "- \"high\": Slower processing, higher detail\n",
    "- \"auto\": The model decides based on the image size\n",
    "\n",
    "Here's how to use the detail parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c174d9d-345d-473c-8a8e-ed317dff232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the detail parameter when analyzing an image with GPT-4 Vision\n",
    "\n",
    "# Text prompt\n",
    "prompt = \"How much calories are in this food?\"\n",
    "\n",
    "# Model\n",
    "openai_model = \"gpt-4o\"\n",
    "\n",
    "# Creating an API request\n",
    "response = client.chat.completions.create(\n",
    "  model=openai_model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": ramen_image_url,\n",
    "            \"detail\": \"low\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "choice = response.choices[0]\n",
    "print(choice.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc35b0a-d434-48bf-8d70-b2f9dec245c8",
   "metadata": {},
   "source": [
    "## Interpreting and Utilizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0f342-adcc-4317-b756-e531658985ef",
   "metadata": {},
   "source": [
    "When interpreting the results from GPT-4 Vision, keep in mind its capabilities and limitations:\n",
    "\n",
    "- The model excels at general descriptions and identifying objects in images.\n",
    "- It can understand relationships between objects but may struggle with precise spatial reasoning.\n",
    "- The model may provide approximate counts for objects in images.\n",
    "- It may have difficulty with very small text or highly specialized images (e.g., medical scans).\n",
    "\n",
    "Here's an example of how to extract specific information from the model's response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334ea1a2-43b8-46d4-8b5d-6aeae5b27905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting specific information when analyzing an image from GPT-4 Vision\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class FoodCalories(BaseModel):\n",
    "    total_calories: str\n",
    "    analysis: str\n",
    "\n",
    "# Use JSON format to make extracting information easier\n",
    "\n",
    "# Text prompt\n",
    "prompt = \"How much calories are in this food?\"\n",
    "\n",
    "# Model\n",
    "openai_model = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "# Creating an API request\n",
    "response = client.beta.chat.completions.parse(\n",
    "  model=openai_model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": ramen_image_url,\n",
    "            \"detail\": \"low\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  response_format=FoodCalories,\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "choice = response.choices[0]\n",
    "print(choice.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d5577-88b5-429f-a72a-67bd0409edf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

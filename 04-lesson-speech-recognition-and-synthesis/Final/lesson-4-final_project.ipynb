{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19c9e08-d6d2-49f0-aa00-58e1a1cd91d0",
   "metadata": {},
   "source": [
    "#  Lesson 4 Project: Speech Recognition and Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7331c4-d35c-4368-9a7b-75fcbed01255",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4014a6a-b366-4144-b3a7-9258a5650a3c",
   "metadata": {},
   "source": [
    "Welcome to Lesson 4 of our course on cloud-based AI applications! Today, you're diving into the exciting world of speech technologies, focusing on speech recognition and speech synthesis.\n",
    "\n",
    "In this lesson, you'll explore two powerful capabilities provided by OpenAI:\n",
    "- Speech Recognition using the Whisper model\n",
    "- Text-to-Speech (TTS) synthesis\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Implement speech recognition using OpenAI's Whisper model\n",
    "- Utilize OpenAI's text-to-speech capabilities for audio synthesis\n",
    "- Design a basic voice interaction feature in an application\n",
    "\n",
    "You'll start by looking at how to convert spoken language into written text using the Whisper model. Then, you'll flip the process and learn how to generate natural-sounding speech from text. Finally, you'll combine these technologies to create a simple but powerful voice interaction feature.\n",
    "\n",
    "Get ready to give your applications a voice and ears!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ee900-82c8-4ed3-8f55-53b1d12e6065",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24651b0-e3da-461b-9d91-fd9e571358eb",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512dee94-a26f-44f7-8e61-2f1e1dd0b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries\n",
    "!pip install openai python-dotenv matplotlib librosa\n",
    "\n",
    "# Load the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up relevant environment variables\n",
    "# Make sure OPENAI_API_KEY=... exists in .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the OpenAI connection object\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32575e02-7de3-4695-83ce-7b6e585ae704",
   "metadata": {},
   "source": [
    "## Implementing Speech recognition using OpenAI's Whisper model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3913d9-2757-48b7-8c8e-ccd9c20a39bd",
   "metadata": {},
   "source": [
    "OpenAI's Whisper model is a powerful tool for speech recognition. First, you must prepare the audio files. You can get the audio input directly by using the microphone on your computer and record it directly inside this Jupyter Notebook. You can also download free sample audio files from [Pixabay](https://pixabay.com/sound-effects/search/audio-files/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873d867-e70a-4d65-a76e-5c4286ca898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load an audio file using librosa\n",
    "\n",
    "# Import libraries\n",
    "import requests\n",
    "import io\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# URL of the sample audio file\n",
    "speech_download_link = \"https://cdn.pixabay.com/download/audio/2022/03/10/audio_a8e603753c.mp3?filename=self-destruct-sequence-31505.mp3\"\n",
    "\n",
    "# Local path where the audio file will be saved\n",
    "save_path = \"audio/self-destruct-sequence.mp3\"\n",
    "\n",
    "# Download the audio file\n",
    "response = requests.get(speech_download_link)\n",
    "if response.status_code == 200:\n",
    "    audio_data = io.BytesIO(response.content)\n",
    "\n",
    "    # Save the audio file locally\n",
    "    with open(save_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Load the audio file using librosa\n",
    "    y, sr = librosa.load(audio_data)\n",
    "\n",
    "    # Display the audio file so it can be played\n",
    "    audio = Audio(data=y, rate=sr, autoplay=True)\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27283681-c8be-4d3e-b732-c47f0c244c23",
   "metadata": {},
   "source": [
    "Create a function to play the audio file. This will be useful for confirming the content of the audio files before transcribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14261c-797a-4963-898d-72a6e44b13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to play the audio file\n",
    "\n",
    "def play_speech(file_path):\n",
    "    # Load the audio file using librosa\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # Create an Audio object for playback\n",
    "    audio = Audio(data=y, rate=sr, autoplay=True)\n",
    "\n",
    "    # Display the audio player\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1192cf-894b-44e0-9152-252ebe0fdc32",
   "metadata": {},
   "source": [
    "Now, use the Whisper model to transcribe the audio file to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89bd384-d398-461c-8d80-04e2877cece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the audio file using the Whisper model\n",
    "\n",
    "with open(save_path, \"rb\") as audio_file:\n",
    "    # Transcribe the audio file using the Whisper model\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      response_format=\"json\"\n",
    "    )\n",
    "# Print the transcription result in JSON format\n",
    "print(transcription.json())\n",
    "# Print only the transcribed text\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d7413-3ae7-488e-ac56-27992cc657c7",
   "metadata": {},
   "source": [
    "To transcribe audio using the Whisper model, you use the `client.audio.transcriptions.create` method, which requires specific parameters to be passed in the request. The `file` parameter is mandatory and must contain the actual audio file object in formats like flac, mp3, wav, or similar—not just the file name. The `model` parameter specifies the ID of the transcription model. The `whisper-1` model is currently the only available model, so it's a required field. The format of the transcription output can be specified with `response_format`. It defaults to `json`. As you can see, with the `json` response format, the JSON result is concise.\n",
    "\n",
    "But you can try another value, `verbose_json`, which will make the JSON result very verbose.\n",
    "\n",
    "There is also another useful parameter, `timestamp_granularities`, to get the timestamps at the word or segment level.\n",
    "\n",
    "You can try to experiment with them with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979ac0d-4b49-4c36-a3c6-e4b1f7f794a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the detailed information with timestamps\n",
    "\n",
    "with open(save_path, \"rb\") as audio_file:\n",
    "    # Transcribe the audio file with word-level timestamps\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      response_format=\"verbose_json\",\n",
    "      timestamp_granularities=[\"word\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7fe3bd-5895-401b-9ba9-81e8e1abdabc",
   "metadata": {},
   "source": [
    "Then you can take a look at the verbose JSON result. You can get much more information from this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb298ad-ed1c-4242-93be-b6b847e26a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the detailed information for each word timestamp\n",
    "\n",
    "import json\n",
    "\n",
    "json_result = transcription.json()\n",
    "print(json_result)\n",
    "\n",
    "json_object = json.loads(json_result)\n",
    "print(json_object[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad185225-ebb2-4b4a-8958-59cbfad35906",
   "metadata": {},
   "source": [
    "You can access detailed information about individual words in the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425cad-8d68-43a1-b5c5-53c08d08e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the detailed information for words\n",
    "\n",
    "# Print the detailed information for each word\n",
    "print(transcription.words)\n",
    "# Print the detailed information for the first two words\n",
    "print(transcription.words[0])\n",
    "print(transcription.words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0631e-0e71-4b0d-b562-d9b7e89b5e1f",
   "metadata": {},
   "source": [
    "You can also obtain segment-level timestamps for the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fa8cd-1858-4911-be66-e76c704a9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the detailed information with segment-level timestamps\n",
    "\n",
    "with open(save_path, \"rb\") as audio_file:\n",
    "    # Transcribe the audio file with segment-level timestamps\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      response_format=\"verbose_json\",\n",
    "      timestamp_granularities=[\"segment\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c29ae-6940-49f8-a25a-f25b377e63b7",
   "metadata": {},
   "source": [
    "You can access detailed information about segments in the transcription. A segment can be something like a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ecb68-ff77-47fc-9bc8-ba9adbd0522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the detailed information for the first two segments\n",
    "print(transcription.segments[0])\n",
    "print(transcription.segments[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fa8e4-e8a5-45b5-9244-fa9efdb52c17",
   "metadata": {},
   "source": [
    "You want to test the transcription with another audio file that has specific words that can be misspelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5532f55-ed6b-4917-b9cd-e8089f76977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & play kodeco-speech.mp3 audio file\n",
    "\n",
    "# Path to another audio file\n",
    "ai_programming_audio_path = \"audio/kodeco-speech.mp3\"\n",
    "# Play the audio file\n",
    "play_speech(ai_programming_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded71b3-243a-4ff5-a01f-988aa9246c8e",
   "metadata": {},
   "source": [
    "You would hear Kodeco and RayWenderlich being mentioned. You want to transcribe the speech again. This time, you want to use the `text` response format which is simpler than the `json` response format. The return result is just the transcription text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa03979-45e1-4c8f-a502-2f264ac6883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the audio file with `text` response format\n",
    "\n",
    "with open(ai_programming_audio_path, \"rb\") as audio_file:\n",
    "    # Transcribe the audio file to text\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      response_format=\"text\"\n",
    "    )\n",
    "# Print the transcribed text\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6838fd1-04a2-4349-ab59-b6aa4d1547f8",
   "metadata": {},
   "source": [
    "But the transcription is wrong. Kodeco and RayWenderlich are misspelled. Fortunately, you can guide the transcription process with the `prompt` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cc031-3c94-42d2-8364-c5b83dca5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the audio file with a prompt to improve accuracy\n",
    "\n",
    "with open(ai_programming_audio_path, \"rb\") as audio_file:\n",
    "    # Transcribe the audio file with a prompt to improve accuracy\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      response_format=\"text\",\n",
    "      prompt=\"Kodeco,RayWenderlich\"\n",
    "    )\n",
    "# Print the transcribed text\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9e115-10aa-44a9-bb75-b8a504d9b2dd",
   "metadata": {},
   "source": [
    "Now, it works well. With the `prompt` parameter, you may guide the transcription with a text prompt, especially useful for continuing a previous segment or matching a specific style. In this case, you guided the transcription in correcting specific words.\n",
    "\n",
    "There is another parameter, `temperature`. This controls how deterministic or random the transcription will be. Lower values (like 0.2) make the transcription more focused, while higher values introduce more randomness. You can experiment it with longer audio file if you are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476f148-5647-4230-be1c-7e1aeb510112",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249191c6-34b2-4a2f-9f67-dff8397dc8bc",
   "metadata": {},
   "source": [
    "Other than transcription, you can also translate the audio file directly to English. Right now, only English is supported.\n",
    "\n",
    "For a start, you want to listen to the Japanese audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42f630-6f57-4d88-b7cb-318338f9dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & play japanese-speech.mp3 audio file\n",
    "\n",
    "# The speech in Japanese: いらっしゃいませ。ラーメン屋へようこそ。何をご注文なさいますか？\n",
    "# Path to the Japanese audio file\n",
    "japanese_audio_path = \"audio/japanese-speech.mp3\"\n",
    "# Play the Japanese audio file\n",
    "play_speech(japanese_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f6ba98-7a48-4648-bbb2-5c1c5c820ab2",
   "metadata": {},
   "source": [
    "To translate, you can use the `client.audio.translation.create` method. The `model`, `file`, and `response format` work the same as in the `client.audio.transcription.create` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74ef14-2798-4ee5-a2b2-1e6d9ca765a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the Japanese audio to English text\n",
    "\n",
    "with open(japanese_audio_path, \"rb\") as audio_file:\n",
    "    # Translate the Japanese audio to English text\n",
    "    translation = client.audio.translations.create(\n",
    "      model=\"whisper-1\", \n",
    "      file=audio_file,\n",
    "      response_format=\"text\"\n",
    "    )\n",
    "# Print the translated text\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62c5a4-9ca2-40a4-b25c-9d43995b9556",
   "metadata": {},
   "source": [
    "## Using OpenAI's Text-To-Speech (TTS) Capabilities for Audio Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74e289-379d-4672-a67f-fae2d45b3a16",
   "metadata": {},
   "source": [
    "To create a synthesis speech, you can use the `client.audio.speech.create` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a69ab-0c56-436d-b203-ce7c107ceb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate speech from text using OpenAI's TTS model\n",
    "\n",
    "# Path to save the synthesized speech\n",
    "speech_file_path = \"audio/learn-ai.mp3\"\n",
    "\n",
    "# Generate speech from text using OpenAI's TTS model\n",
    "with client.audio.speech.with_streaming_response.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Would you like to learn AI programming? We have many AI programming courses that you can choose.\"\n",
    ") as response:\n",
    "  # Save the synthesized speech to the specified path\n",
    "  response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39827ae4-29d9-4e3b-b9f9-d83ad368f4c1",
   "metadata": {},
   "source": [
    "The model parameter is set to \"tts-1\", specifying the text-to-speech model to be used. This model is optimized for speed. You can use another model, \"tts-1-hd\", if you care more about the quality. The voice parameter is set to \"alloy\", which determines the voice characteristics such as tone and accent. You have other choices, like `echo`, `fable`, `onyx`, `nova`, and `shimmer`. Finally, the input parameter contains the text that you want to convert to speech: \"Would you like to learn AI programming? We have many AI programming courses that you can choose.\".\n",
    "\n",
    "You want to play the synthesized speech to hear the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d3d82-a80a-4250-84b0-25d6f9d64918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the synthesized speech\n",
    "play_speech(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bdfdb-62fe-4fdc-b78b-a3f3d99315e5",
   "metadata": {},
   "source": [
    "Nice! You've created a synthesized speech.\n",
    "\n",
    "You can experiment with another value for the `voice` parameter. There is also another parameter, `speed`. You can make the speed of the speech slower by giving it the value less than 1 or make it faster by giving it the value greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c3dff-580a-4762-8fa0-904a725e50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate speech with a different voice and slower speed\n",
    "\n",
    "# Generate speech with a different voice and slower speed\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"echo\",\n",
    "  speed=0.6,\n",
    "  input=\"Would you like to learn AI programming? We have many AI programming courses that you can choose.\"\n",
    ")\n",
    "\n",
    "# Save the synthesized speech to the specified path\n",
    "response.stream_to_file(speech_file_path)\n",
    "\n",
    "# Play the synthesized speech\n",
    "play_speech(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac65f24-dac4-4126-a955-3e35322d20c5",
   "metadata": {},
   "source": [
    "## Designing a Basic Voice Interaction Feature in an Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3ad3b-5dec-4929-90cc-c348d2d7b1c1",
   "metadata": {},
   "source": [
    "Now, you want to combine speech recognition and synthesis to create a simple language tutor application. This application will listen to the user speak in a language, check if the grammar is correct, and provide feedback using synthesized speech.\n",
    "\n",
    "First, you will define a function to transcribe the recorded speech using the Whisper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792738e-5973-4003-bd08-12b87ab2af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transcribe the recorded speech\n",
    "\n",
    "def transcript_speech(speech_filename=\"my_speech.m4a\"):\n",
    "    with open(speech_filename, \"rb\") as audio_file:\n",
    "        # Open the audio file and transcribe using the Whisper model\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "          model=\"whisper-1\", \n",
    "          file=audio_file,\n",
    "          response_format=\"json\",\n",
    "          language=\"en\"\n",
    "        )\n",
    "    # Return the transcribed text\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991c18e-4126-46d2-a584-a1e63e4d206c",
   "metadata": {},
   "source": [
    "You will also need a function to check the grammar of the transcribed text using OpenAI's GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d1779-1204-4c71-ab80-ee3a267e184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the grammar of the transcribed text\n",
    "\n",
    "def check_grammar(english_text):\n",
    "    # Use GPT to check and correct the grammar of the input text\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an English grammar expert.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Fix the grammar: {english_text}\"}\n",
    "      ]\n",
    "    )\n",
    "    # Extract and return the corrected grammar message\n",
    "    message = response.choices[0].message.content\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11ec57-5ca7-40f8-82fc-bd4a4caf7700",
   "metadata": {},
   "source": [
    "Next, you need a function to generate spoken feedback using the text-to-speech capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886eedb8-debc-4dd6-a236-b129eecd048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide spoken feedback using TTS\n",
    "\n",
    "def tell_feedback(grammar_feedback, speech_file_path=\"feedback_speech.mp3\"):\n",
    "    # Generate speech from the grammar feedback using TTS\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=grammar_feedback\n",
    "    )\n",
    "\n",
    "    # Save the synthesized speech to the specified path\n",
    "    response.stream_to_file(speech_file_path)\n",
    "    # Play the synthesized speech\n",
    "    play_speech(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae0d08-b467-48be-8d97-7a3b2fa5d276",
   "metadata": {},
   "source": [
    "Finally, put everything together in a function that handles the entire process from recording audio to providing spoken feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8392-c2f6-4938-b216-c0918ff1a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the grammar feedback application\n",
    "\n",
    "def grammar_feedback_app(speech_filename):\n",
    "    # Transcribe the recorded speech\n",
    "    transcription = transcript_speech(speech_filename)\n",
    "    print(transcription)\n",
    "    # Check and correct the grammar of the transcription\n",
    "    feedback = check_grammar(transcription)\n",
    "    print(feedback)\n",
    "    # Provide spoken feedback using TTS\n",
    "    tell_feedback(feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4d5c9-9e10-4c5d-b1dd-83f6467a3c09",
   "metadata": {},
   "source": [
    "To create an audio file for speech input, you can use the Sound Recorder app on Windows, QuickTime on Mac, or a similar recording application on Linux. Once recorded, place the audio file in the `audio` folder and update the `wrong_grammar_audio` variable accordingly. Alternatively, you can use a provided audio sample containing a grammatically incorrect sentence, \"My sister don't like to eat on night,\" for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189a5de-1d93-47ac-bc99-c8a97ab6611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the audio file. Use the audio sample or record the audio yourself and place the file here.\n",
    "wrong_grammar_audio = \"audio/grammar-wrong.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88eddf4-9558-4b26-bd17-acf79f379575",
   "metadata": {},
   "source": [
    "You can listen to the audio input file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74552a7d-aa44-4fae-8374-410d8607b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the grammatically wrong audio file\n",
    "play_speech(wrong_grammar_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646ac3e-6627-4949-91a3-0a95e348c181",
   "metadata": {},
   "source": [
    "Then run the grammar feedback application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea793ab8-7a78-4c42-9083-be67ce2625b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grammar feedback application\n",
    "grammar_feedback_app(wrong_grammar_audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19c9e08-d6d2-49f0-aa00-58e1a1cd91d0",
   "metadata": {},
   "source": [
    "#  Lesson 4 Project: Speech Recognition and Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7331c4-d35c-4368-9a7b-75fcbed01255",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4014a6a-b366-4144-b3a7-9258a5650a3c",
   "metadata": {},
   "source": [
    "Welcome to Lesson 4 of our course on cloud-based AI applications! Today, you're diving into the exciting world of speech technologies, focusing on speech recognition and speech synthesis.\n",
    "\n",
    "In this lesson, you'll explore two powerful capabilities provided by OpenAI:\n",
    "- Speech Recognition using the Whisper model\n",
    "- Text-to-Speech (TTS) synthesis\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Implement speech recognition using OpenAI's Whisper model\n",
    "- Utilize OpenAI's text-to-speech capabilities for audio synthesis\n",
    "- Design a basic voice interaction feature in an application\n",
    "\n",
    "You'll start by looking at how to convert spoken language into written text using the Whisper model. Then, you'll flip the process and learn how to generate natural-sounding speech from text. Finally, you'll combine these technologies to create a simple but powerful voice interaction feature.\n",
    "\n",
    "Get ready to give your applications a voice and ears!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ee900-82c8-4ed3-8f55-53b1d12e6065",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24651b0-e3da-461b-9d91-fd9e571358eb",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512dee94-a26f-44f7-8e61-2f1e1dd0b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries\n",
    "!pip install openai python-dotenv matplotlib librosa\n",
    "\n",
    "# Load the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up relevant environment variables\n",
    "# Make sure OPENAI_API_KEY=... exists in .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the OpenAI connection object\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32575e02-7de3-4695-83ce-7b6e585ae704",
   "metadata": {},
   "source": [
    "## Implementing Speech recognition using OpenAI's Whisper model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3913d9-2757-48b7-8c8e-ccd9c20a39bd",
   "metadata": {},
   "source": [
    "OpenAI's Whisper model is a powerful tool for speech recognition. First, you must prepare the audio files. You can get the audio input directly by using the microphone on your computer and record it directly inside this Jupyter Notebook. You can also download free sample audio files from [Pixabay](https://pixabay.com/sound-effects/search/audio-files/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873d867-e70a-4d65-a76e-5c4286ca898a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27283681-c8be-4d3e-b732-c47f0c244c23",
   "metadata": {},
   "source": [
    "Create a function to play the audio file. This will be useful for confirming the content of the audio files before transcribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14261c-797a-4963-898d-72a6e44b13df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac1192cf-894b-44e0-9152-252ebe0fdc32",
   "metadata": {},
   "source": [
    "Now, use the Whisper model to transcribe the audio file to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89bd384-d398-461c-8d80-04e2877cece1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3d7413-3ae7-488e-ac56-27992cc657c7",
   "metadata": {},
   "source": [
    "To transcribe audio using the Whisper model, you use the `client.audio.transcriptions.create` method, which requires specific parameters to be passed in the request. The `file` parameter is mandatory and must contain the actual audio file object in formats like flac, mp3, wav, or similarâ€”not just the file name. The `model` parameter specifies the ID of the transcription model. The `whisper-1` model is currently the only available model, so it's a required field. The format of the transcription output can be specified with `response_format`. It defaults to `json`. As you can see, with the `json` response format, the JSON result is concise.\n",
    "\n",
    "But you can try another value, `verbose_json`, which will make the JSON result very verbose.\n",
    "\n",
    "There is also another useful parameter, `timestamp_granularities`, to get the timestamps at the word or segment level.\n",
    "\n",
    "You can try to experiment with them with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979ac0d-4b49-4c36-a3c6-e4b1f7f794a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7fe3bd-5895-401b-9ba9-81e8e1abdabc",
   "metadata": {},
   "source": [
    "Then you can take a look at the verbose JSON result. You can get much more information from this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb298ad-ed1c-4242-93be-b6b847e26a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad185225-ebb2-4b4a-8958-59cbfad35906",
   "metadata": {},
   "source": [
    "You can access detailed information about individual words in the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425cad-8d68-43a1-b5c5-53c08d08e414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58e0631e-0e71-4b0d-b562-d9b7e89b5e1f",
   "metadata": {},
   "source": [
    "You can also obtain segment-level timestamps for the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fa8cd-1858-4911-be66-e76c704a9038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5c29ae-6940-49f8-a25a-f25b377e63b7",
   "metadata": {},
   "source": [
    "You can access detailed information about segments in the transcription. A segment can be something like a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ecb68-ff77-47fc-9bc8-ba9adbd0522a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318fa8e4-e8a5-45b5-9244-fa9efdb52c17",
   "metadata": {},
   "source": [
    "You want to test the transcription with another audio file that has specific words that can be misspelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5532f55-ed6b-4917-b9cd-e8089f76977d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ded71b3-243a-4ff5-a01f-988aa9246c8e",
   "metadata": {},
   "source": [
    "You would hear Kodeco and RayWenderlich being mentioned. You want to transcribe the speech again. This time, you want to use the `text` response format which is simpler than the `json` response format. The return result is just the transcription text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa03979-45e1-4c8f-a502-2f264ac6883a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6838fd1-04a2-4349-ab59-b6aa4d1547f8",
   "metadata": {},
   "source": [
    "But the transcription is wrong. Kodeco and RayWenderlich are misspelled. Fortunately, you can guide the transcription process with the `prompt` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cc031-3c94-42d2-8364-c5b83dca5509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04a9e115-10aa-44a9-bb75-b8a504d9b2dd",
   "metadata": {},
   "source": [
    "Now, it works well. With the `prompt` parameter, you may guide the transcription with a text prompt, especially useful for continuing a previous segment or matching a specific style. In this case, you guided the transcription in correcting specific words.\n",
    "\n",
    "There is another parameter, `temperature`. This controls how deterministic or random the transcription will be. Lower values (like 0.2) make the transcription more focused, while higher values introduce more randomness. You can experiment it with longer audio file if you are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476f148-5647-4230-be1c-7e1aeb510112",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249191c6-34b2-4a2f-9f67-dff8397dc8bc",
   "metadata": {},
   "source": [
    "Other than transcription, you can also translate the audio file directly to English. Right now, only English is supported.\n",
    "\n",
    "For a start, you want to listen to the Japanese audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42f630-6f57-4d88-b7cb-318338f9dccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70f6ba98-7a48-4648-bbb2-5c1c5c820ab2",
   "metadata": {},
   "source": [
    "To translate, you can use the `client.audio.translation.create` method. The `model`, `file`, and `response format` work the same as in the `client.audio.transcription.create` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74ef14-2798-4ee5-a2b2-1e6d9ca765a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc62c5a4-9ca2-40a4-b25c-9d43995b9556",
   "metadata": {},
   "source": [
    "## Using OpenAI's Text-To-Speech (TTS) Capabilities for Audio Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74e289-379d-4672-a67f-fae2d45b3a16",
   "metadata": {},
   "source": [
    "To create a synthesis speech, you can use the `client.audio.speech.create` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a69ab-0c56-436d-b203-ce7c107ceb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39827ae4-29d9-4e3b-b9f9-d83ad368f4c1",
   "metadata": {},
   "source": [
    "The model parameter is set to \"tts-1\", specifying the text-to-speech model to be used. This model is optimized for speed. You can use another model, \"tts-1-hd\", if you care more about the quality. The voice parameter is set to \"alloy\", which determines the voice characteristics such as tone and accent. You have other choices, like `echo`, `fable`, `onyx`, `nova`, and `shimmer`. Finally, the input parameter contains the text that you want to convert to speech: \"Would you like to learn AI programming? We have many AI programming courses that you can choose.\".\n",
    "\n",
    "You want to play the synthesized speech to hear the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d3d82-a80a-4250-84b0-25d6f9d64918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e93bdfdb-62fe-4fdc-b78b-a3f3d99315e5",
   "metadata": {},
   "source": [
    "Nice! You've created a synthesized speech.\n",
    "\n",
    "You can experiment with another value for the `voice` parameter. There is also another parameter, `speed`. You can make the speed of the speech slower by giving it the value less than 1 or make it faster by giving it the value greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c3dff-580a-4762-8fa0-904a725e50b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ac65f24-dac4-4126-a955-3e35322d20c5",
   "metadata": {},
   "source": [
    "## Designing a Basic Voice Interaction Feature in an Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3ad3b-5dec-4929-90cc-c348d2d7b1c1",
   "metadata": {},
   "source": [
    "Now, you want to combine speech recognition and synthesis to create a simple language tutor application. This application will listen to the user speak in a language, check if the grammar is correct, and provide feedback using synthesized speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c161bf7-5dd9-48ea-ab6c-15bd97fd2ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792738e-5973-4003-bd08-12b87ab2af1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f2d8f-682c-49b8-b587-92379dc6a818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff1e2ff0-5b14-4b29-b525-3eafd7c1581c",
   "metadata": {},
   "source": [
    "Define a couple of helper functions to handle audio input recording and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2efe20-115e-4568-a5b9-8d56f27d333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc472198-d503-465a-a167-eabc1c4382e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, you will define a function to transcribe the recorded speech using the Whisper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6838412-820d-4121-b99a-055361e58fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7991c18e-4126-46d2-a584-a1e63e4d206c",
   "metadata": {},
   "source": [
    "You will also need a function to check the grammar of the transcribed text using OpenAI's GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d1779-1204-4c71-ab80-ee3a267e184e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f11ec57-5ca7-40f8-82fc-bd4a4caf7700",
   "metadata": {},
   "source": [
    "Next, you need a function to generate spoken feedback using the text-to-speech capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886eedb8-debc-4dd6-a236-b129eecd048d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9ae0d08-b467-48be-8d97-7a3b2fa5d276",
   "metadata": {},
   "source": [
    "Finally, put everything together in a function that handles the entire process from recording audio to providing spoken feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8392-c2f6-4938-b216-c0918ff1a963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f4d5c9-9e10-4c5d-b1dd-83f6467a3c09",
   "metadata": {},
   "source": [
    "Initialize the audio recorder and run the grammar feedback application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189a5de-1d93-47ac-bc99-c8a97ab6611b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8646ac3e-6627-4949-91a3-0a95e348c181",
   "metadata": {},
   "source": [
    "Then run the grammar feedback application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea793ab8-7a78-4c42-9083-be67ce2625b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grammar feedback application\n",
    "grammar_feedback_app(recorder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

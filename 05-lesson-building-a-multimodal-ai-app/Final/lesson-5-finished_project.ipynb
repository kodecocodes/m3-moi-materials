{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728a8006-ee9f-4d81-ab40-39f472f2327c",
   "metadata": {},
   "source": [
    "# Lesson 5 Project: Building a Multimodal AI App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ecd2a-bce9-40a5-9ac0-7594a24caef2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1b535-66e6-4300-8452-99b468ea6408",
   "metadata": {},
   "source": [
    "Welcome to Lesson 5, where you'll embark on an exciting journey to create a sophisticated multimodal AI application. In this lesson, you'll build a language tutor app that integrates text, image, and audio processing to provide an immersive and interactive learning experience.\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "* Integrate text, image, and audio processing in a single application\n",
    "* Implement a user interface for multimodal interactions\n",
    "* Evaluate the effectiveness of multimodal integration in enhancing user experience\n",
    "\n",
    "Let's dive in and start building a language tutor app!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea425998-b7c1-466f-b12f-f1cf5b3a8da2",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbf776-4d77-43cd-9d9e-5642c6062477",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f4f2e-efed-4561-8fc7-98b3d08cd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries, including Gradio\n",
    "!pip install openai requests python-dotenv matplotlib librosa ipyaudioworklet gradio Pillow\n",
    "\n",
    "# Load the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up relevant environment variables\n",
    "# Make sure OPENAI_API_KEY=... exists in .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the OpenAI connection object\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c754e-a2a8-420d-ac26-cc92ec2db5bd",
   "metadata": {},
   "source": [
    "## Using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96394fc-7099-4813-9247-c05814a3df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214c825-6397-46eb-b4e2-933d483b9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name, is_morning, temperature):\n",
    "    salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
    "    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n",
    "    celsius = (temperature - 32) * 5 / 9\n",
    "    return greeting, round(celsius, 2)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n",
    "    outputs=[\"text\", \"number\"],\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cd8fa-85f8-45e3-9715-0f2899dbcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769, 0.189],\n",
    "        [0.349, 0.686, 0.168],\n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(), \"image\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58f9a9-e896-4bbe-b38a-a9ba236997da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator(num1, operation, num2):\n",
    "    if operation == \"add\":\n",
    "        return num1 + num2\n",
    "    elif operation == \"subtract\":\n",
    "        return num1 - num2\n",
    "    elif operation == \"multiply\":\n",
    "        return num1 * num2\n",
    "    elif operation == \"divide\":\n",
    "        if num2 == 0:\n",
    "            raise gr.Error(\"Cannot divide by zero!\")\n",
    "        return num1 / num2\n",
    "\n",
    "demo = gr.Interface(\n",
    "    calculator,\n",
    "    [\n",
    "        \"number\",\n",
    "        gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]),\n",
    "        \"number\"\n",
    "    ],\n",
    "    \"number\",\n",
    "    examples=[\n",
    "        [45, \"add\", 3],\n",
    "        [3.14, \"divide\", 2],\n",
    "        [144, \"multiply\", 2.5],\n",
    "        [0, \"subtract\", 1.2],\n",
    "    ],\n",
    "    title=\"Toy Calculator\",\n",
    "    description=\"Here's a sample toy calculator.\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088e49e-6f88-474d-8a23-efecdc3cfe00",
   "metadata": {},
   "source": [
    "## Generating Situational Prompts and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867add-9391-43f9-8d3e-55f99be36538",
   "metadata": {},
   "source": [
    "Let's create a function to generate a situational prompt and a corresponding image using OpenAI's GPT-4 and DALL-E models. For example, the results could be:\n",
    "- The \"A person is ordering a cafe latte in a coffee shop\" situational prompt (generated by OpenAI's GPT-4)\n",
    "- An image of a person ordering a cafe latte in a coffee shop (generated by DALL-E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3996715-3730-42c6-a4b2-7447a91e41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_situational_prompt(seed_prompt=\"\"):\n",
    "    additional_prompt = \"\"\"\n",
    "    Then create an initial response to the person. If the situation is \"ordering coffee in a cafe.\", then\n",
    "        the initial response will be, \"Hello, what would you like to order?\".\n",
    "        Seperate the initial situation and the initial response with a line containing \"====\". Something like:\n",
    "        \"You're ordering coffee in a cafe.\n",
    "        ====\n",
    "        'Hello, there. What would you like to order?'\"\n",
    "        Limit the output to 1 sentence.\n",
    "    \"\"\"\n",
    "    if seed_prompt:\n",
    "        seed_phrase = f\"\"\"Generate a second-person POV situation for practicing English with this seed prompt: {seed_prompt}.\n",
    "        {additional_prompt}\"\"\"\n",
    "    else:\n",
    "        seed_phrase = f\"\"\"Generate a second-person POV situation for practicing English, like meeting your parents-in-law, etc.\n",
    "        {additional_prompt}\"\"\"\n",
    "    # Use GPT to generate a situation for practicing English\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writer. Very very creative.\"},\n",
    "        {\"role\": \"user\", \"content\": seed_phrase}\n",
    "      ]\n",
    "    )\n",
    "    # Extract and return the situation and the initial response\n",
    "    message = response.choices[0].message.content\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10855d5c-bbf1-4866-ac95-3f9523214885",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_situational_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f51c1d-d5b0-435a-ab7d-92972900bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_situation_image(dalle_prompt):\n",
    "    response = client.images.generate(\n",
    "      model=\"dall-e-3\",\n",
    "      prompt=dalle_prompt,\n",
    "      size=\"1024x1024\",\n",
    "      n=1,\n",
    "    )\n",
    "    image_url = response.data[0].url\n",
    "    response = requests.get(image_url)\n",
    "\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    return img, image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867916b-4681-4605-b6bc-6b609c32b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ac008-cdc5-4e8b-959d-7000aab2694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = generate_situational_prompt(\"cafe\")\n",
    "initial_situation_prompt = full_response.split('====')[0].strip()\n",
    "print(initial_situation_prompt)\n",
    "img = generate_situation_image(initial_situation_prompt)\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc5839-58e5-44d3-954c-1f25e4ffffc5",
   "metadata": {},
   "source": [
    "## Implementing Speech Recognition and Speech Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76627eee-4297-44cd-8be4-adcce4d5a8bd",
   "metadata": {},
   "source": [
    "In this section, you'll generate a text prompt using OpenAI's GPT-4 API, such as \"Welcome to Cute Cafe. What do you want to order?\" The text then must be sent to OpenAI's TTS API so you'll have the synthesis voice that you must play.\n",
    "\n",
    "Then you need to wait a user's voice or audio input, such as, \"I want to order a cafe latte.\" This audio file must be sent to OpenAI's Whisper API so for transcription so in the end you'll have the text from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33404405-be74-450a-871b-41148cfa5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def play_speech(file_path):\n",
    "    # Load the audio file using librosa\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # Create an Audio object for playback\n",
    "    audio = Audio(data=y, rate=sr, autoplay=True)\n",
    "\n",
    "    # Display the audio player\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1aed84-5ed5-4a2d-933a-54b480b6fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_prompt(speech_prompt, autoplay=True, speech_file_path=\"speech.mp3\"):\n",
    "    # Generate speech from the grammar feedback using TTS\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=speech_prompt\n",
    "    )\n",
    "\n",
    "    # Save the synthesized speech to the specified path\n",
    "    response.stream_to_file(speech_file_path)\n",
    "\n",
    "    if autoplay:\n",
    "        # Play the synthesized speech\n",
    "        play_speech(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8f52b-e4e4-4d67-b505-ba5efc4b632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_response = full_response.split('====')[1].strip()\n",
    "speak_prompt(initial_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3acabc-73f8-47c2-8d1a-8bfd0f09f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyaudioworklet as ipyaudio\n",
    "import wave\n",
    "\n",
    "def receive_audio_input(speech_filename=\"my_speech.wav\"):\n",
    "    # Create an audio recorder object\n",
    "    recorder = ipyaudio.AudioRecorder(filename=speech_filename)\n",
    "    return recorder\n",
    "\n",
    "def save_recorded_audio_input(recorder):\n",
    "    # Save the recorded audio to a file\n",
    "    _x = (recorder.audiodata * 32767.5).astype(dtype=np.int16)\n",
    "    with wave.open(recorder.filename, mode='wb') as wb:\n",
    "         wb.setnchannels(1)\n",
    "         wb.setsampwidth(_x.itemsize)\n",
    "         wb.setframerate(recorder.sampleRate)\n",
    "         wb.writeframes(_x.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae658b-cf72-4392-bd9d-db689999fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_speech(speech_filename=\"my_speech.wav\"):\n",
    "    with open(speech_filename, \"rb\") as audio_file:\n",
    "        # Open the audio file and transcribe using the Whisper model\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "          model=\"whisper-1\", \n",
    "          file=audio_file,\n",
    "          response_format=\"json\",\n",
    "          language=\"en\"\n",
    "        )\n",
    "    # Return the transcribed text\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b8ce8-7cdc-4f03-b8f2-4f94583d9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = receive_audio_input()\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190b040-54ed-47bd-8956-d91d089b4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_recorded_audio_input(recorder)\n",
    "transcripted_text = transcript_speech()\n",
    "print(transcripted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66715f9-d13d-49c2-9334-39d63be9e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_conversation_history(history, added_response):\n",
    "    history = f\"\"\"{history}\n",
    "====\n",
    "'{added_response}'\n",
    "\"\"\"\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745191c-9ba9-48f5-a8ca-67e0bb8a3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = creating_conversation_history(full_response, transcripted_text)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97569dd-ddfa-430d-969c-45646ab556fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversation_from_history(history):\n",
    "    prompt = \"\"\"Continue conversation from a person based on this conversation history and end it with '\\n====\\n'.\n",
    "    Limit it to max 3 sentences.\n",
    "    This is the history:\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writer. Very very creative.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\\n{history}\"}\n",
    "      ]\n",
    "    )\n",
    "    # Extract and return the situation and the initial response\n",
    "    message = response.choices[0].message.content\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88233141-bf09-4076-b10c-8064025571f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = generate_conversation_from_history(history)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ade9b6-7d26-4772-b877-e3e66bee5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_history = history + \"\\n====\\n\" + conversation\n",
    "print(combined_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aede27-79fe-4c76-ac3f-ec35894d5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle_prompt = \"Generate a scenery based on this conversation: \" + combined_history\n",
    "img = generate_situation_image(dalle_prompt)\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86bae2d-97f3-4599-90b2-4c60180d55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_prompt(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb378a-8d72-49df-a594-3a1d82c2a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate a text prompt based on the situational prompt with OpenAI's GPT-4 API.\n",
    "\n",
    "# Create a function to send this text prompt to OpenAI's TTS API then play the audio file to the user.\n",
    "\n",
    "# Create a function to receive audio input from a user.\n",
    "# You can use PyAudio to record it in the Jupyter notebook.\n",
    "# Or you can record it in a separate occassion then provide the path to the audio file.\n",
    "\n",
    "# Create a function to send the audio file to OpenAI's Whisper for transcription and get the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c347bd-5133-48eb-8f0c-2eb3c7821456",
   "metadata": {},
   "source": [
    "## Building the User Interface with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487cc68-de5a-4c9c-a51f-78d414d8510b",
   "metadata": {},
   "source": [
    "Now, let's create your multimodal language tutor app using Gradio. At first, when the app is launched, there will be an image showed to the user,\n",
    "such as an image of a cafe. Then there will be an audio file being played, such as, \"Welcome to Cute Cafe. What do you like to order?\"\n",
    "\n",
    "There will be an interface to record speech from the user, \"I would like to have a cup of cafe latte.\". As an alternative, you can also have an interface to upload the audio file.\n",
    "\n",
    "Then the image will be changed to another image, such as, an image of of a cafe latte. Then there will be speech generated and played to the user, \"Would you like another thing, such as croissant?\"\n",
    "\n",
    "The user can give an answer, \"No, but what is the wifi password?\" Then the image will be changed to another image, such as an image of a wifi router or a note displaying the wifi password. And so on. You get the idea.\n",
    "\n",
    "A user can use this app until they get bored. There is a button to quit the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa403bbb-51d2-4fdd-be1a-66b0e8ea1f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_situation = generate_situational_prompt(\"cafe near beach\")\n",
    "img = generate_situation_image(initial_situation)\n",
    "\n",
    "first_time = True\n",
    "combined_history = \"\"\n",
    "\n",
    "def extract_first_last(text):\n",
    "    elements = [elem.strip() for elem in text.split('====') if elem.strip()]\n",
    "\n",
    "    if len(elements) >= 2:\n",
    "        return elements[0] + elements[-1]\n",
    "    elif len(elements) == 1:\n",
    "        return elements[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def conversation_generation(audio_path):\n",
    "    global combined_history\n",
    "    global first_time\n",
    "    transcripted_text = transcript_speech(audio_path)\n",
    "    if first_time:\n",
    "        history = creating_conversation_history(initial_situation, transcripted_text)\n",
    "        first_time = False\n",
    "    else:\n",
    "        history = creating_conversation_history(combined_history, transcripted_text)\n",
    "    print(history)\n",
    "    conversation = generate_conversation_from_history(history)\n",
    "    combined_history = history + \"\\n====\\n\" + conversation\n",
    "    dalle_prompt = extract_first_last(combined_history)\n",
    "    img = generate_situation_image(combined_history)\n",
    "    output_audio_file = \"speak_speech.mp3\"\n",
    "    speak_prompt(conversation, False, output_audio_file)\n",
    "\n",
    "    return img, conversation, output_audio_file\n",
    "\n",
    "tutor_app = gr.Interface(\n",
    "    conversation_generation,\n",
    "    gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
    "    outputs=[gr.Image(value=img), gr.Text(), gr.Audio(type=\"filepath\")],\n",
    "    title=\"Speaking Language Tutor App\",\n",
    "    description=initial_situation\n",
    ")\n",
    "\n",
    "tutor_app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135a895-d5f7-4434-9baa-762c8ae901d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window\n",
    "# Display an image in the window\n",
    "# Play the audio file in the window\n",
    "# Create an interface to record speech or to upload the audio file\n",
    "# Create a way to regenerate the image and replay a different speech\n",
    "# Create a button to quit the app\n",
    "# Integrate the user interface with the core OpenAI API functions that you created previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a3331-7629-47ed-8c03-6df4d402bafd",
   "metadata": {},
   "source": [
    "## Evaluation and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacbdf3-7feb-4691-ab1a-48ae2aeae14d",
   "metadata": {},
   "source": [
    "After building and testing your multimodal AI app, consider the following questions:\n",
    "\n",
    "1. How does the integration of text, image, and audio enhance the language learning experience?\n",
    "2. What challenges did you face in designing the user interface for multi-modal interactions?\n",
    "3. How might you improve the app to make it more effective or user-friendly?\n",
    "\n",
    "Take some time to reflect on these questions and discuss your thoughts with your peers or instructor.\n",
    "\n",
    "Also, you can try to build the multimodal AI app outside Jupyter notebook. Put the app inside a Python script so you can run it in command line interface or terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3f388-9140-4ac9-9f61-a758c25dbf32",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e5002-fc78-4527-b89d-a7f2139bc5e9",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully built a multimodal AI application that integrates text, image, and audio processing. This language tutor app demonstrates the powerful potential of combining multiple AI technologies to create an immersive and interactive learning experience.\n",
    "\n",
    "Remember, this is just the beginning. There are many ways to expand and improve this app, such as implementing more sophisticated speech recognition, adding more scenarios, or incorporating user feedback to improve the AI's responses.\n",
    "\n",
    "Keep exploring and experimenting with multimodal AI applications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0455af7-c7a1-44be-9531-8d58835752d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728a8006-ee9f-4d81-ab40-39f472f2327c",
   "metadata": {},
   "source": [
    "# Lesson 5 Project: Building a Multimodal AI App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ecd2a-bce9-40a5-9ac0-7594a24caef2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1b535-66e6-4300-8452-99b468ea6408",
   "metadata": {},
   "source": [
    "Welcome to Lesson 5, where you'll embark on an exciting journey to create a sophisticated multimodal AI application. In this lesson, you'll build a language tutor app that integrates text, image, and audio processing to provide an immersive and interactive learning experience.\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "* Integrate text, image, and audio processing in a single application\n",
    "* Implement a user interface for multimodal interactions\n",
    "* Evaluate the effectiveness of multimodal integration in enhancing user experience\n",
    "\n",
    "Let's dive in and start building a language tutor app!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea425998-b7c1-466f-b12f-f1cf5b3a8da2",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbf776-4d77-43cd-9d9e-5642c6062477",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment.\n",
    "\n",
    "In this lesson, you also need to install the gradio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657f4f2e-efed-4561-8fc7-98b3d08cd111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (1.40.6)\n",
      "Requirement already satisfied: requests in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: librosa in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: gradio in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (4.43.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: fastapi<0.113.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.112.4)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.24.6)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.6.4)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio) (0.30.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio-client==1.3.0->gradio) (2024.9.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from fastapi<0.113.0->gradio) (0.38.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.16.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install the libraries, including Gradio\n",
    "!pip install openai requests python-dotenv matplotlib librosa gradio Pillow\n",
    "\n",
    "# Load the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up relevant environment variables\n",
    "# Make sure OPENAI_API_KEY=... exists in .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the OpenAI connection object\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c754e-a2a8-420d-ac26-cc92ec2db5bd",
   "metadata": {},
   "source": [
    "## Using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85aeaa0-73c5-4d33-a1e2-801f3c15d433",
   "metadata": {},
   "source": [
    "Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share your demo with a public link in seconds using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience needed!\n",
    "\n",
    "In this section, you will learn how to create Gradio applications in Jupyter Lab. Every time you execute Gradio code in a cell, it will launch a Gradio app in a new port. You should start with a simple Gradio app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96394fc-7099-4813-9247-c05814a3df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Gradio library\n",
    "import gradio as gr\n",
    "\n",
    "# Define a simple function that takes a name and a time of day as inputs\n",
    "def greet(name, greeting_time):\n",
    "    return \"Good \" +  greeting_time + \", \" + name + \"!\"\n",
    "\n",
    "# Create a Gradio interface for the function\n",
    "demo = gr.Interface(\n",
    "    fn=greet,  # The function to wrap a UI around\n",
    "    inputs=[gr.Text(), gr.Dropdown([\"morning\", \"evening\", \"night\"])],  # Define input components\n",
    "    outputs=[gr.Text()], # Define output components\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dbdc0-1b5f-4ca9-a2ca-28c226b9ed45",
   "metadata": {},
   "source": [
    "In this example, you created a simple Gradio interface that takes a name and a time of day (morning, evening, night) as inputs and returns a greeting message. You can see that the number of inputs in the function matches the number of input components, and the number of return values matches the number of output components.\n",
    "\n",
    "But you don't have to limit the output components to only one component. It can be more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214c825-6397-46eb-b4e2-933d483b9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a greeting message and a hard-coded image URL\n",
    "def greet(name, greeting_time):\n",
    "    greeting = \"Good \" +  greeting_time + \", \" + name + \"!\"\n",
    "    image_url = \"https://upload.wikimedia.org/wikipedia/commons/d/d6/An_Oberoi_Hotel_employee_doing_Namaste%2C_New_Delhi.jpg\"\n",
    "    return (greeting, image_url)\n",
    "\n",
    "# Create a Gradio interface for the function\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[gr.Text(), gr.Dropdown([\"morning\", \"evening\", \"night\"])],\n",
    "    outputs=[gr.Text(), gr.Image()],\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef210b-9550-407f-9bef-56fa11864381",
   "metadata": {},
   "source": [
    "In this example, you have enhanced the function to return not only a greeting message but also an image URL. Gradio handles displaying the image using the `gr.Image()` component. The number of return values from the function matches the number of output components\n",
    "\n",
    "You can also use an audio input field and an audio output field. For the audio input, you can use the microphone as the source of audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a3843-a3c4-4065-8c6b-1aef0720b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a greeting message, an image URL, and an audio file path\n",
    "def greet(name, greeting_time, audio_path):\n",
    "    greeting = \"Good \" +  greeting_time + \", \" + name + \"!\"\n",
    "    image_url = \"https://upload.wikimedia.org/wikipedia/commons/d/d6/An_Oberoi_Hotel_employee_doing_Namaste%2C_New_Delhi.jpg\"\n",
    "    return (greeting, image_url, audio_path)\n",
    "\n",
    "# Create a Gradio interface for the function\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[gr.Text(), gr.Dropdown([\"morning\", \"evening\", \"night\"]), gr.Audio(sources=[\"microphone\"], type=\"filepath\")],\n",
    "    outputs=[gr.Text(), gr.Image(), gr.Audio(type=\"filepath\")],\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a420c3-816e-436b-a3a5-cd830ea0dbc3",
   "metadata": {},
   "source": [
    "In this example, you added an audio input component using `gr.Audio()` using the `sources` and `type` arguments that allows users to provide audio input through a microphone and passes the audio input to the function as the audio file path. The function now returns a greeting message, an image URL, and the audio file path.\n",
    "\n",
    "You can also make your app more informative using the `title` and `description` arguments in the `gr.Interface` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58f9a9-e896-4bbe-b38a-a9ba236997da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns a greeting message, an image URL, and an audio file path\n",
    "def greet(name, greeting_time, audio_path):\n",
    "    greeting = \"Good \" +  greeting_time + \", \" + name + \"!\"\n",
    "    image_url = \"https://upload.wikimedia.org/wikipedia/commons/d/d6/An_Oberoi_Hotel_employee_doing_Namaste%2C_New_Delhi.jpg\"\n",
    "    return (greeting, image_url, audio_path)\n",
    "\n",
    "# Create a Gradio interface for the function with a title and description\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[gr.Text(), gr.Dropdown([\"morning\", \"evening\", \"night\"]), gr.Audio(sources=[\"microphone\"], type=\"filepath\")],\n",
    "    outputs=[gr.Text(), gr.Image(), gr.Audio(type=\"filepath\")],\n",
    "    title=\"Greeting App\",\n",
    "    description=\"This is a billion dollar greeting app.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abf864-7957-4cdb-9fa0-0150191ce596",
   "metadata": {},
   "source": [
    "In this final example, you added a title and description to the Gradio interface. These elements help users understand the purpose of the app and provide context for the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088e49e-6f88-474d-8a23-efecdc3cfe00",
   "metadata": {},
   "source": [
    "## Generating Situational Prompts and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867add-9391-43f9-8d3e-55f99be36538",
   "metadata": {},
   "source": [
    "Before you create a multimodal AI app with UI, you need to create functions to generate a situation where users can practice the English language, generate a scenery image of the situation, and the initial response that triggers a conversation between users and AI.\n",
    "\n",
    "You start with the function to generate the initial situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3996715-3730-42c6-a4b2-7447a91e41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_situational_prompt(seed_prompt=\"\"):\n",
    "    # Define additional prompt instructions\n",
    "    additional_prompt = \"\"\"\n",
    "    Then create an initial response to the person. If the situation is \"ordering coffee in a cafe.\", then\n",
    "        the initial response will be, \"Hello, what would you like to order?\".\n",
    "        Seperate the initial situation and the initial response with a line containing \"====\". Something like:\n",
    "        \"You're ordering coffee in a cafe.\n",
    "        ====\n",
    "        'Hello, there. What would you like to order?'\"\n",
    "        Limit the output to 1 sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if a seed prompt is provided and create the seed phrase accordingly\n",
    "    if seed_prompt:\n",
    "        seed_phrase = f\"\"\"Generate a second-person POV situation for practicing English with this seed prompt: {seed_prompt}.\n",
    "        {additional_prompt}\"\"\"\n",
    "    else:\n",
    "        seed_phrase = f\"\"\"Generate a second-person POV situation for practicing English, like meeting your parents-in-law, etc.\n",
    "        {additional_prompt}\"\"\"\n",
    "\n",
    "    # Use GPT to generate a situation for practicing English\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writer. Very very creative.\"},\n",
    "        {\"role\": \"user\", \"content\": seed_phrase}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    # Extract and return the situation and the initial response from the response\n",
    "    message = response.choices[0].message.content\n",
    "\n",
    "    # Return the generated message\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911fa40-f4fc-478d-907d-7365a7f44e0e",
   "metadata": {},
   "source": [
    "Test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10855d5c-bbf1-4866-ac95-3f9523214885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You\\'re meeting your parents-in-law for the first time at a family dinner.\\n====\\n\"Hello, it\\'s wonderful to finally meet you both; I\\'ve heard so much about you.\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_situational_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd803be0-7e8f-44f9-8832-baadfe5d51a6",
   "metadata": {},
   "source": [
    "Test the function with the seed prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccb95bd-c8eb-4da4-ab05-42d8853023b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are attending a comics exhibition.\\n====\\n\"Welcome to the exhibition! Which artist would you like to learn more about today?\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_situational_prompt(\"comics exhibition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa58aff-1c70-4c3d-8b46-98e343bbd079",
   "metadata": {},
   "source": [
    "To enhance the immersive experience of practicing English, you can generate a scenery image that matches the situation. For example, if the situation is \"ordering coffee in a cafe,\" you can generate an image of a cafe. This helps in visualizing the context, making the practice more engaging and realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f51c1d-d5b0-435a-ab7d-92972900bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing and display\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def generate_situation_image(dalle_prompt):\n",
    "    # Generate an image using the DALL-E 3 model with the provided prompt\n",
    "    response = client.images.generate(\n",
    "      model=\"dall-e-3\", # Specify the model to use\n",
    "      prompt=dalle_prompt, # The prompt describing the image to generate\n",
    "      size=\"1024x1024\", # Specify the size of the generated image\n",
    "      n=1, # Number of images to generate\n",
    "    )\n",
    "\n",
    "    # Retrieve the URL of the generated image\n",
    "    image_url = response.data[0].url\n",
    "\n",
    "    # Download the image from the URL\n",
    "    response = requests.get(image_url)\n",
    "\n",
    "     # Open the image using PIL\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Return the image object\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a867916b-4681-4605-b6bc-6b609c32b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the image in the cell\n",
    "def display_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf4107-c143-40ce-9881-17b0ba6a7400",
   "metadata": {},
   "source": [
    "Now that we have both functions ready, you can execute them together. The image generation function requires the output from the text generation function first. In this example, you'll create a situation related to a \"cafe\" and then generate an image based on that situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ac008-cdc5-4e8b-959d-7000aab2694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = generate_situational_prompt(\"cafe\")\n",
    "initial_situation_prompt = full_response.split('====')[0].strip()\n",
    "print(initial_situation_prompt)\n",
    "img = generate_situation_image(initial_situation_prompt)\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc5839-58e5-44d3-954c-1f25e4ffffc5",
   "metadata": {},
   "source": [
    "## Implementing Speech Recognition and Speech Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76627eee-4297-44cd-8be4-adcce4d5a8bd",
   "metadata": {},
   "source": [
    "After implementing functions for text and image generation, you will explore how to implement functions to handle speech recognition and speech synthesis. In the multimodal app, you want to get the audio input from the user and give back the audio response. Remember this is the app for practicing conversation in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33404405-be74-450a-871b-41148cfa5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for audio processing and display\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Function to play a speech file\n",
    "def play_speech(file_path):\n",
    "    # Load the audio file using librosa\n",
    "    y, sr = librosa.load(file_path)\n",
    "\n",
    "    # Create an Audio object for playback\n",
    "    audio = Audio(data=y, rate=sr, autoplay=True)\n",
    "\n",
    "    # Display the audio player\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f84c3-dfbb-4c7b-898f-192c69fb63ae",
   "metadata": {},
   "source": [
    "Next, create a function to generate speech from a text prompt using a Text-to-Speech (TTS) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1aed84-5ed5-4a2d-933a-54b480b6fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate speech from a text prompt\n",
    "def speak_prompt(speech_prompt, autoplay=True, speech_file_path=\"speech.mp3\"):\n",
    "    # Generate speech from the grammar feedback using TTS\n",
    "    response = client.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",\n",
    "      input=speech_prompt\n",
    "    )\n",
    "\n",
    "    # Save the synthesized speech to the specified path\n",
    "    response.stream_to_file(speech_file_path)\n",
    "\n",
    "    # Sometimes you want to play the speech automatically, sometimes you do not\n",
    "    if autoplay:\n",
    "        # Play the synthesized speech\n",
    "        play_speech(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf85ca-ca72-4775-bd74-4ee41c91f09a",
   "metadata": {},
   "source": [
    "Now, extract the initial situation from the generated situational prompt and use the `speak_prompt` function to generate and play the speech for this initial response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8f52b-e4e4-4d67-b505-ba5efc4b632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_situation = full_response.split('====')[1].strip()\n",
    "speak_prompt(initial_situation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f4680-ac11-49e3-865d-7ffa4625406f",
   "metadata": {},
   "source": [
    "Next, create a function to transcribe the speech into text using a speech-to-text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbae658b-cf72-4392-bd9d-db689999fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe speech from an audio file\n",
    "def transcript_speech(speech_filename=\"my_speech.wav\"):\n",
    "    with open(speech_filename, \"rb\") as audio_file:\n",
    "        # Transcribe the audio file using the Whisper model\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "          model=\"whisper-1\", \n",
    "          file=audio_file,\n",
    "          response_format=\"json\",\n",
    "          language=\"en\"\n",
    "        )\n",
    "    # Return the transcribed text\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6210710b-987a-48cf-8422-0dd1c1921397",
   "metadata": {},
   "source": [
    "Transcribe the speech. Then, print the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b190b040-54ed-47bd-8956-d91d089b4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to order a cup of cappuccino.\n"
     ]
    }
   ],
   "source": [
    "# Transcribe the audio\n",
    "transcripted_text = transcript_speech(\"audio/cappuccino.m4a\")\n",
    "\n",
    "# Print the transcribed text\n",
    "print(transcripted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c15a08-912a-4add-ad07-a25ba989c664",
   "metadata": {},
   "source": [
    "Create a conversation history by combining the initial response and the transcribed text. Here is the function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e66715f9-d13d-49c2-9334-39d63be9e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a conversation history\n",
    "def creating_conversation_history(history, added_response):\n",
    "    history = f\"\"\"{history}\n",
    "====\n",
    "'{added_response}'\n",
    "\"\"\"\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f7fae-bd8e-4e3d-b448-a96f1a540908",
   "metadata": {},
   "source": [
    "Now use the function to create and print the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b745191c-9ba9-48f5-a8ca-67e0bb8a3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're asking a stranger in a cafe if you can share their table.\n",
      "====\n",
      "\"Hi, do you mind if I sit here?\"\n",
      "====\n",
      "'I would like to order a cup of cappuccino.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = creating_conversation_history(full_response, transcripted_text)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa60e1-7b41-4fcb-8897-658433d74ef2",
   "metadata": {},
   "source": [
    "Next, generate a continuation of the conversation based on the conversation history. Here is the function to generate the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e97569dd-ddfa-430d-969c-45646ab556fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a conversation based on the conversation history\n",
    "def generate_conversation_from_history(history):\n",
    "    prompt = \"\"\"Continue conversation from a person based on this conversation history and end it with '\\n====\\n'.\n",
    "    Limit it to max 3 sentences.\n",
    "    This is the history:\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writer. Very very creative.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\\n{history}\"}\n",
    "      ]\n",
    "    )\n",
    "    # Extract and return the generated conversation\n",
    "    message = response.choices[0].message.content\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05baa9bb-3306-4384-b969-b9ba60ada985",
   "metadata": {},
   "source": [
    "Generate and print the conversation based on the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88233141-bf09-4076-b10c-8064025571f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Of course, go ahead. I'm about to order one myself—cappuccino's my favorite.\"\n"
     ]
    }
   ],
   "source": [
    "# Generate and print the conversation based on the history\n",
    "conversation = generate_conversation_from_history(history)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf6f62-2f0b-43af-966c-969a4ae32b82",
   "metadata": {},
   "source": [
    "Combine the conversation history with the new conversation and print the combined history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ade9b6-7d26-4772-b877-e3e66bee5e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're asking a stranger in a cafe if you can share their table.\n",
      "====\n",
      "\"Hi, do you mind if I sit here?\"\n",
      "====\n",
      "'I would like to order a cup of cappuccino.'\n",
      "\n",
      "====\n",
      "\"Of course, go ahead. I'm about to order one myself—cappuccino's my favorite.\"\n"
     ]
    }
   ],
   "source": [
    "# Combine the conversation history with the new conversation\n",
    "combined_history = history + \"\\n====\\n\" + conversation\n",
    "\n",
    "# Print the combined history\n",
    "print(combined_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab32e8-8db9-41ce-9ae3-eb56103a38a3",
   "metadata": {},
   "source": [
    "Next, generate a scenery image based on the combined history using the `generate_situation_image` function and display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aede27-79fe-4c76-ac3f-ec35894d5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scenery image based on the combined history\n",
    "dalle_prompt = \"Generate a scenery based on this conversation: \" + combined_history\n",
    "img = generate_situation_image(dalle_prompt)\n",
    "\n",
    "# Display the generated image\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fa48c-5322-42eb-ac06-43eee19e6e11",
   "metadata": {},
   "source": [
    "Finally, generate and play the prompt based on the new conversation using the `speak_prompt` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86bae2d-97f3-4599-90b2-4c60180d55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_prompt(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c347bd-5133-48eb-8f0c-2eb3c7821456",
   "metadata": {},
   "source": [
    "## Building the User Interface with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487cc68-de5a-4c9c-a51f-78d414d8510b",
   "metadata": {},
   "source": [
    "Now, let's create your multimodal language tutor app using Gradio. When the app is launched, it will display an image to the user, such as a picture of a cafe. An audio file will then play, for example, \"Welcome to Cute Cafe. What would you like to order?\"\n",
    "\n",
    "There will be an interface for the user to record their speech, such as, \"I would like to have a cup of cafe latte.\"\n",
    "\n",
    "The image will then change to another picture, for instance, an image of a cafe latte. Speech will be generated and played to the user, saying, \"Would you like anything else, such as a croissant?\"\n",
    "\n",
    "The user can respond, \"No, but what is the wifi password?\" The image will change again, perhaps to a picture of a wifi router or a note displaying the wifi password. And so on. You get the idea.\n",
    "\n",
    "The user can use this app until they decide to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa403bbb-51d2-4fdd-be1a-66b0e8ea1f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial seed prompt for generating the initial situational context\n",
    "seed_prompt = \"cafe near beach\" # or \"comics exhibition\", \"meeting parents-in-law for the first time\", etc\n",
    "\n",
    "# Generate an initial situational description based on the seed prompt\n",
    "initial_situation = generate_situational_prompt(seed_prompt)\n",
    "\n",
    "# Generate an initial image based on the initial situational description\n",
    "img = generate_situation_image(initial_situation)\n",
    "\n",
    "# Flags to manage the state of the app\n",
    "first_time = True\n",
    "combined_history = \"\"\n",
    "\n",
    "# Function to extract the first and last segments of the conversation history\n",
    "# This is to ensure that the prompt for DALL-E does not exceed the maximum character limit of 4000 characters\n",
    "def extract_first_last(text):\n",
    "    elements = [elem.strip() for elem in text.split('====') if elem.strip()]\n",
    "\n",
    "    if len(elements) >= 2:\n",
    "        return elements[0] + elements[-1]\n",
    "    elif len(elements) == 1:\n",
    "        return elements[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Main function to handle the conversation generation logic\n",
    "def conversation_generation(audio_path):\n",
    "    global combined_history\n",
    "    global first_time\n",
    "\n",
    "    # Transcribe the user's speech from the provided audio file path\n",
    "    transcripted_text = transcript_speech(audio_path)\n",
    "\n",
    "    # Create conversation history based on whether it is the first interaction or not\n",
    "    if first_time:\n",
    "        history = creating_conversation_history(initial_situation, transcripted_text)\n",
    "        first_time = False\n",
    "    else:\n",
    "        history = creating_conversation_history(combined_history, transcripted_text)\n",
    "\n",
    "    # Generate a new conversation based on the updated history\n",
    "    conversation = generate_conversation_from_history(history)\n",
    "\n",
    "    # Update the combined history with the new conversation\n",
    "    combined_history = history + \"\\n====\\n\" + conversation\n",
    "    \n",
    "    # Extract a suitable prompt for DALL-E by combining the first and last parts of the conversation history\n",
    "    dalle_prompt = extract_first_last(combined_history)\n",
    "\n",
    "    # Generate a new image based on the updated combined history\n",
    "    img = generate_situation_image(combined_history)\n",
    "\n",
    "    # Generate speech for the new conversation and save it to an audio file\n",
    "    output_audio_file = \"speak_speech.mp3\"\n",
    "    speak_prompt(conversation, False, output_audio_file)\n",
    "\n",
    "    # Return the updated image, conversation text, and audio file path\n",
    "    return img, conversation, output_audio_file\n",
    "\n",
    "# Create the Gradio interface for the language tutor app\n",
    "tutor_app = gr.Interface(\n",
    "    conversation_generation,\n",
    "    gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
    "    outputs=[gr.Image(value=img), gr.Text(), gr.Audio(type=\"filepath\")],\n",
    "    title=\"Speaking Language Tutor App\",\n",
    "    description=initial_situation\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "tutor_app.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e712a-3665-4a23-81af-dc5cc356a0b2",
   "metadata": {},
   "source": [
    "This multimodal language tutor app helps users practice language skills through interactive scenarios. When the app starts, it displays an image and displays an initial prompt related to a specific scenario, such as a cafe near a beach. Users can respond by recording their speech. The app transcribes their speech, updates the conversation history, generates new responses, and updates the visual and audio outputs accordingly.\n",
    "\n",
    "### Inputs and Outputs\n",
    "\n",
    "- Inputs:\n",
    "  - Audio file (recorded via microphone)\n",
    "- Outputs:\n",
    "  - Image (updated based on conversation context)\n",
    "  - Text (generated conversation response)\n",
    "  - Audio file (generated speech response)\n",
    "\n",
    "### Flow of the Program\n",
    "\n",
    "1. Initialization:\n",
    "   - The app starts with a seed prompt (e.g., \"cafe near beach\").\n",
    "   - An initial situational description and corresponding image are generated.\n",
    "2. User interaction:\n",
    "   - The user records an audio file with their response.\n",
    "   - The app transcribes the audio to text.\n",
    "3. Conversation Update:\n",
    "   - The app updates the conversation history with the new user input.\n",
    "   - A new conversation response is generated based on the updated history.\n",
    "   - The history is preserved and updated for future interactions.\n",
    "4. Visual and Audio Update:\n",
    "   - A new image is generated based on the updated history.\n",
    "   - New speech is generated from the conversation response and saved to an audio file.\n",
    "5. Outputs:\n",
    "   - The updated image, conversation text, and speech audio are displayed and played to the user.\n",
    "  \n",
    "### State Preservation\n",
    "\n",
    "To preserve the state in the Gradio app, global variables (first_time and combined_history) are used. These variables keep track of whether it is the first interaction and the combined history of the conversation, respectively. This allows the app to maintain the context of the conversation across multiple interactions, ensuring a coherent and continuous dialogue with the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a3331-7629-47ed-8c03-6df4d402bafd",
   "metadata": {},
   "source": [
    "## Evaluation and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacbdf3-7feb-4691-ab1a-48ae2aeae14d",
   "metadata": {},
   "source": [
    "The app isn't perfect. It doesn't maintain consistency with characters in the generated images. For example, if you order a cup of coffee, one time you might be greeted by a man, and the next time by a woman. Additionally, it doesn't provide grammar feedback to what you say in the conversation.\n",
    "\n",
    "The effectiveness of multimodal integration in enhancing user experience involves using different types of media—like text, audio, images, and interactive elements—to make interactions more engaging and easier to understand. By combining these various forms of communication, apps can meet the needs and preferences of a broader range of users. For example, an app that uses visual instructions along with spoken feedback can help users learn and remember information better, while also making the experience more enjoyable. This approach can also improve accessibility, such as providing text descriptions for images or audio transcriptions for videos, making the app usable for people with disabilities. Overall, the aim is to see if these combined methods lead to happier users, more engagement, and a smoother, more enjoyable experience.\n",
    "\n",
    "After building and testing your multimodal AI app, consider the following questions:\n",
    "\n",
    "1. How does the integration of text, image, and audio enhance the language learning experience?\n",
    "2. What challenges did you face in designing the user interface for multi-modal interactions?\n",
    "3. How might you improve the app to make it more effective or user-friendly?\n",
    "\n",
    "Take some time to reflect on these questions and discuss your thoughts with your peers or instructor.\n",
    "\n",
    "Also, you can try to build the multimodal AI app outside Jupyter notebook. Put the app inside a Python script so you can run it in command line interface or terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da9e53-50a3-408a-b4fb-a79b400fa4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

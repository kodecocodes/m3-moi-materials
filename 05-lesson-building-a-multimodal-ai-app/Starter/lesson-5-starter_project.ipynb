{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728a8006-ee9f-4d81-ab40-39f472f2327c",
   "metadata": {},
   "source": [
    "# Lesson 5 Project: Building a Multimodal AI App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ecd2a-bce9-40a5-9ac0-7594a24caef2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1b535-66e6-4300-8452-99b468ea6408",
   "metadata": {},
   "source": [
    "Welcome to Lesson 5, where you'll embark on an exciting journey to create a sophisticated multimodal AI application. In this lesson, you'll build a language tutor app that integrates text, image, and audio processing to provide an immersive and interactive learning experience.\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "* Integrate text, image, and audio processing in a single application\n",
    "* Implement a user interface for multimodal interactions\n",
    "* Evaluate the effectiveness of multimodal integration in enhancing user experience\n",
    "\n",
    "Let's dive in and start building a language tutor app!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea425998-b7c1-466f-b12f-f1cf5b3a8da2",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbf776-4d77-43cd-9d9e-5642c6062477",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment.\n",
    "\n",
    "In this lesson, you also need to install the gradio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f4f2e-efed-4561-8fc7-98b3d08cd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "\n",
    "# Load the OpenAI library\n",
    "\n",
    "# Set up relevant environment variables\n",
    "# Make sure OPENAI_API_KEY=... exists in .env\n",
    "\n",
    "# Create the OpenAI connection object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c754e-a2a8-420d-ac26-cc92ec2db5bd",
   "metadata": {},
   "source": [
    "## Using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85aeaa0-73c5-4d33-a1e2-801f3c15d433",
   "metadata": {},
   "source": [
    "Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share your demo with a public link in seconds using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience needed!\n",
    "\n",
    "In this section, you will learn how to create Gradio applications in Jupyter Lab. Every time you execute Gradio code in a cell, it will launch a Gradio app in a new port. You should start with a simple Gradio app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96394fc-7099-4813-9247-c05814a3df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradio interface to greet the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dbdc0-1b5f-4ca9-a2ca-28c226b9ed45",
   "metadata": {},
   "source": [
    "In this example, you created a simple Gradio interface that takes a name and a time of day (morning, evening, night) as inputs and returns a greeting message. You can see that the number of inputs in the function matches the number of input components, and the number of return values matches the number of output components.\n",
    "\n",
    "But you don't have to limit the output components to only one component. It can be more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214c825-6397-46eb-b4e2-933d483b9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an app that takes in a name and time of day and returns a greeting message and an image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef210b-9550-407f-9bef-56fa11864381",
   "metadata": {},
   "source": [
    "In this example, you have enhanced the function to return not only a greeting message but also an image URL. Gradio handles displaying the image using the `gr.Image()` component. The number of return values from the function matches the number of output components\n",
    "\n",
    "You can also use an audio input field and an audio output field. For the audio input, you can use the microphone as the source of audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a3843-a3c4-4065-8c6b-1aef0720b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an app that takes in a name and time of day and returns a greeting message, an image, and an audio file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a420c3-816e-436b-a3a5-cd830ea0dbc3",
   "metadata": {},
   "source": [
    "In this example, you added an audio input component using `gr.Audio()` using the `sources` and `type` arguments that allows users to provide audio input through a microphone and passes the audio input to the function as the audio file path. The function now returns a greeting message, an image URL, and the audio file path.\n",
    "\n",
    "You can also make your app more informative using the `title` and `description` arguments in the `gr.Interface` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58f9a9-e896-4bbe-b38a-a9ba236997da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a title and description to the app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abf864-7957-4cdb-9fa0-0150191ce596",
   "metadata": {},
   "source": [
    "In this final example, you added a title and description to the Gradio interface. These elements help users understand the purpose of the app and provide context for the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088e49e-6f88-474d-8a23-efecdc3cfe00",
   "metadata": {},
   "source": [
    "## Generating Situational Prompts and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867add-9391-43f9-8d3e-55f99be36538",
   "metadata": {},
   "source": [
    "Before you create a multimodal AI app with UI, you need to create functions to generate a situation where users can practice the English language, generate a scenery image of the situation, and the initial response that triggers a conversation between users and AI.\n",
    "\n",
    "You start with the function to generate the initial situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3996715-3730-42c6-a4b2-7447a91e41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a situational prompt for practicing English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911fa40-f4fc-478d-907d-7365a7f44e0e",
   "metadata": {},
   "source": [
    "Test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10855d5c-bbf1-4866-ac95-3f9523214885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function to generate a situational prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd803be0-7e8f-44f9-8832-baadfe5d51a6",
   "metadata": {},
   "source": [
    "Test the function with the seed prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb95bd-c8eb-4da4-ab05-42d8853023b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function to generate a situational prompt with a seed prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa58aff-1c70-4c3d-8b46-98e343bbd079",
   "metadata": {},
   "source": [
    "To enhance the immersive experience of practicing English, you can generate a scenery image that matches the situation. For example, if the situation is \"ordering coffee in a cafe,\" you can generate an image of a cafe. This helps in visualizing the context, making the practice more engaging and realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f51c1d-d5b0-435a-ab7d-92972900bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an image based on the situational prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89777c8f-902a-4671-8492-c6bf3637c442",
   "metadata": {},
   "source": [
    "Then, create a function to display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a867916b-4681-4605-b6bc-6b609c32b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image in the cell\n",
    "\n",
    "# Display the image in the cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf4107-c143-40ce-9881-17b0ba6a7400",
   "metadata": {},
   "source": [
    "Now that we have both functions ready, you can execute them together. The image generation function requires the output from the text generation function first. In this example, you'll create a situation related to a \"cafe\" and then generate an image based on that situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ac008-cdc5-4e8b-959d-7000aab2694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the functions to generate a situational prompt and its matching image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc5839-58e5-44d3-954c-1f25e4ffffc5",
   "metadata": {},
   "source": [
    "## Implementing Speech Recognition and Speech Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76627eee-4297-44cd-8be4-adcce4d5a8bd",
   "metadata": {},
   "source": [
    "After implementing functions for text and image generation, you will explore how to implement functions to handle speech recognition and speech synthesis. In the multimodal app, you want to get the audio input from the user and give back the audio response. Remember this is the app for practicing conversation in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33404405-be74-450a-871b-41148cfa5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f84c3-dfbb-4c7b-898f-192c69fb63ae",
   "metadata": {},
   "source": [
    "Next, create a function to generate speech from a text prompt using a Text-to-Speech (TTS) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb1aed84-5ed5-4a2d-933a-54b480b6fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate speech from a text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf85ca-ca72-4775-bd74-4ee41c91f09a",
   "metadata": {},
   "source": [
    "Now, extract the initial situation from the generated situational prompt and use the `speak_prompt` function to generate and play the speech for this initial response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8f52b-e4e4-4d67-b505-ba5efc4b632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the initial response based on the situational prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f4680-ac11-49e3-865d-7ffa4625406f",
   "metadata": {},
   "source": [
    "Next, create a function to transcribe the speech into text using a speech-to-text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbae658b-cf72-4392-bd9d-db689999fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe speech from an audio file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71919c7-5df9-4e32-a7c9-35f580864e1e",
   "metadata": {},
   "source": [
    "Transcribe the speech. Then, print the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190b040-54ed-47bd-8956-d91d089b4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the audio\n",
    "\n",
    "# Print the transcribed text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c15a08-912a-4add-ad07-a25ba989c664",
   "metadata": {},
   "source": [
    "Create a conversation history by combining the initial response and the transcribed text. Here is the function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e66715f9-d13d-49c2-9334-39d63be9e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a conversation history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f7fae-bd8e-4e3d-b448-a96f1a540908",
   "metadata": {},
   "source": [
    "Now use the function to create and print the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745191c-9ba9-48f5-a8ca-67e0bb8a3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and print the conversation history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa60e1-7b41-4fcb-8897-658433d74ef2",
   "metadata": {},
   "source": [
    "Next, generate a continuation of the conversation based on the conversation history. Here is the function to generate the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e97569dd-ddfa-430d-969c-45646ab556fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a conversation based on the conversation history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05baa9bb-3306-4384-b969-b9ba60ada985",
   "metadata": {},
   "source": [
    "Generate and print the conversation based on the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88233141-bf09-4076-b10c-8064025571f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print the conversation based on the history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf6f62-2f0b-43af-966c-969a4ae32b82",
   "metadata": {},
   "source": [
    "Combine the conversation history with the new conversation and print the combined history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ade9b6-7d26-4772-b877-e3e66bee5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the conversation history with the new conversation\n",
    "\n",
    "# Print the combined history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab32e8-8db9-41ce-9ae3-eb56103a38a3",
   "metadata": {},
   "source": [
    "Next, generate a scenery image based on the combined history using the `generate_situation_image` function and display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aede27-79fe-4c76-ac3f-ec35894d5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scenery image based on the combined history\n",
    "\n",
    "\n",
    "# Display the generated image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fa48c-5322-42eb-ac06-43eee19e6e11",
   "metadata": {},
   "source": [
    "Finally, generate and play the prompt based on the new conversation using the `speak_prompt` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86bae2d-97f3-4599-90b2-4c60180d55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and play the prompt based on the new conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c347bd-5133-48eb-8f0c-2eb3c7821456",
   "metadata": {},
   "source": [
    "## Building the User Interface with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487cc68-de5a-4c9c-a51f-78d414d8510b",
   "metadata": {},
   "source": [
    "Now, let's create your multimodal language tutor app using Gradio. When the app is launched, it will display an image to the user, such as a picture of a cafe. An audio file will then play, for example, \"Welcome to Cute Cafe. What would you like to order?\"\n",
    "\n",
    "There will be an interface for the user to record their speech, such as, \"I would like to have a cup of cafe latte.\"\n",
    "\n",
    "The image will then change to another picture, for instance, an image of a cafe latte. Speech will be generated and played to the user, saying, \"Would you like anything else, such as a croissant?\"\n",
    "\n",
    "The user can respond, \"No, but what is the wifi password?\" The image will change again, perhaps to a picture of a wifi router or a note displaying the wifi password. And so on. You get the idea.\n",
    "\n",
    "The user can use this app until they decide to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa403bbb-51d2-4fdd-be1a-66b0e8ea1f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the main function to handle the conversation generation logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e712a-3665-4a23-81af-dc5cc356a0b2",
   "metadata": {},
   "source": [
    "This multimodal language tutor app helps users practice language skills through interactive scenarios. When the app starts, it displays an image and displays an initial prompt related to a specific scenario, such as a cafe near a beach. Users can respond by recording their speech. The app transcribes their speech, updates the conversation history, generates new responses, and updates the visual and audio outputs accordingly.\n",
    "\n",
    "### Inputs and Outputs\n",
    "\n",
    "- Inputs:\n",
    "  - Audio file (recorded via microphone)\n",
    "- Outputs:\n",
    "  - Image (updated based on conversation context)\n",
    "  - Text (generated conversation response)\n",
    "  - Audio file (generated speech response)\n",
    "\n",
    "### Flow of the Program\n",
    "\n",
    "1. Initialization:\n",
    "   - The app starts with a seed prompt (e.g., \"cafe near beach\").\n",
    "   - An initial situational description and corresponding image are generated.\n",
    "2. User interaction:\n",
    "   - The user records an audio file with their response.\n",
    "   - The app transcribes the audio to text.\n",
    "3. Conversation Update:\n",
    "   - The app updates the conversation history with the new user input.\n",
    "   - A new conversation response is generated based on the updated history.\n",
    "   - The history is preserved and updated for future interactions.\n",
    "4. Visual and Audio Update:\n",
    "   - A new image is generated based on the updated history.\n",
    "   - New speech is generated from the conversation response and saved to an audio file.\n",
    "5. Outputs:\n",
    "   - The updated image, conversation text, and speech audio are displayed and played to the user.\n",
    "  \n",
    "### State Preservation\n",
    "\n",
    "To preserve the state in the Gradio app, global variables (first_time and combined_history) are used. These variables keep track of whether it is the first interaction and the combined history of the conversation, respectively. This allows the app to maintain the context of the conversation across multiple interactions, ensuring a coherent and continuous dialogue with the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a3331-7629-47ed-8c03-6df4d402bafd",
   "metadata": {},
   "source": [
    "## Evaluation and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacbdf3-7feb-4691-ab1a-48ae2aeae14d",
   "metadata": {},
   "source": [
    "The app isn't perfect. It doesn't maintain consistency with characters in the generated images. For example, if you order a cup of coffee, one time you might be greeted by a man, and the next time by a woman. Additionally, it doesn't provide grammar feedback to what you say in the conversation.\n",
    "\n",
    "The effectiveness of multimodal integration in enhancing user experience involves using different types of media—like text, audio, images, and interactive elements—to make interactions more engaging and easier to understand. By combining these various forms of communication, apps can meet the needs and preferences of a broader range of users. For example, an app that uses visual instructions along with spoken feedback can help users learn and remember information better, while also making the experience more enjoyable. This approach can also improve accessibility, such as providing text descriptions for images or audio transcriptions for videos, making the app usable for people with disabilities. Overall, the aim is to see if these combined methods lead to happier users, more engagement, and a smoother, more enjoyable experience.\n",
    "\n",
    "After building and testing your multimodal AI app, consider the following questions:\n",
    "\n",
    "1. How does the integration of text, image, and audio enhance the language learning experience?\n",
    "2. What challenges did you face in designing the user interface for multi-modal interactions?\n",
    "3. How might you improve the app to make it more effective or user-friendly?\n",
    "\n",
    "Take some time to reflect on these questions and discuss your thoughts with your peers or instructor.\n",
    "\n",
    "Also, you can try to build the multimodal AI app outside Jupyter notebook. Put the app inside a Python script so you can run it in command line interface or terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da9e53-50a3-408a-b4fb-a79b400fa4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

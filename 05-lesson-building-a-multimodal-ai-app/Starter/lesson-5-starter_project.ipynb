{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728a8006-ee9f-4d81-ab40-39f472f2327c",
   "metadata": {},
   "source": [
    "# Lesson 5 Project: Building a Multimodal AI App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ecd2a-bce9-40a5-9ac0-7594a24caef2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1b535-66e6-4300-8452-99b468ea6408",
   "metadata": {},
   "source": [
    "Welcome to Lesson 5, where you'll embark on an exciting journey to create a sophisticated multimodal AI application. In this lesson, you'll build a language tutor app that integrates text, image, and audio processing to provide an immersive and interactive learning experience.\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "* Integrate text, image, and audio processing in a single application\n",
    "* Implement a user interface for multimodal interactions\n",
    "* Evaluate the effectiveness of multimodal integration in enhancing user experience\n",
    "\n",
    "Let's dive in and start building a language tutor app!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea425998-b7c1-466f-b12f-f1cf5b3a8da2",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbf776-4d77-43cd-9d9e-5642c6062477",
   "metadata": {},
   "source": [
    "Refer to the Python Crash Course lesson to learn how to set up your OpenAI development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657f4f2e-efed-4561-8fc7-98b3d08cd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI library\n",
    "# Set up relevant environment variables\n",
    "# Create the OpenAI connection object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1defb31b-f74f-4ec8-9f75-91c4d6a8af6c",
   "metadata": {},
   "source": [
    "In this lesson, you'll use the Gradio library to build the user interface for your multimodal AI app. So install the Gradio library first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7dd14a-88ea-476b-a542-e4d11a3a5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Gradio library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088e49e-6f88-474d-8a23-efecdc3cfe00",
   "metadata": {},
   "source": [
    "## Generating Situational Prompts and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867add-9391-43f9-8d3e-55f99be36538",
   "metadata": {},
   "source": [
    "Let's create a function to generate a situational prompt and a corresponding image using OpenAI's GPT-4 and DALL-E models. For example, the results could be:\n",
    "- The \"A person is ordering a cafe latte in a coffee shop\" situational prompt (generated by OpenAI's GPT-4)\n",
    "- An image of a person ordering a cafe latte in a coffee shop (generated by DALL-E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f895eb-99cd-4921-a342-696c8cf31aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate a situational prompt for practising a target language with OpenAI's GPT-4 API\n",
    "# Create a function to generate a corresponding image with DALL-E API\n",
    "# Combine those functions into one function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc5839-58e5-44d3-954c-1f25e4ffffc5",
   "metadata": {},
   "source": [
    "## Implementing Speech Recognition and Speech Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76627eee-4297-44cd-8be4-adcce4d5a8bd",
   "metadata": {},
   "source": [
    "In this section, you'll generate a text prompt using OpenAI's GPT-4 API, such as \"Welcome to Cute Cafe. What do you want to order?\" The text then must be sent to OpenAI's TTS API so you'll have the synthesis voice that you must play.\n",
    "\n",
    "Then you need to wait a user's voice or audio input, such as, \"I want to order a cafe latte.\" This audio file must be sent to OpenAI's Whisper API so for transcription so in the end you'll have the text from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03bb378a-8d72-49df-a594-3a1d82c2a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate a text prompt based on the situational prompt with OpenAI's GPT-4 API.\n",
    "\n",
    "# Create a function to send this text prompt to OpenAI's TTS API then play the audio file to the user.\n",
    "\n",
    "# Create a function to receive audio input from a user.\n",
    "# You can use PyAudio to record it in the Jupyter notebook.\n",
    "# Or you can record it in a separate occassion then provide the path to the audio file.\n",
    "\n",
    "# Create a function to send the audio file to OpenAI's Whisper for transcription and get the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c347bd-5133-48eb-8f0c-2eb3c7821456",
   "metadata": {},
   "source": [
    "## Building the User Interface with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487cc68-de5a-4c9c-a51f-78d414d8510b",
   "metadata": {},
   "source": [
    "Now, let's create your multimodal language tutor app using Gradio. At first, when the app is launched, there will be an image showed to the user,\n",
    "such as an image of a cafe. Then there will be an audio file being played, such as, \"Welcome to Cute Cafe. What do you like to order?\"\n",
    "\n",
    "There will be an interface to record speech from the user, \"I would like to have a cup of cafe latte.\". As an alternative, you can also have an interface to upload the audio file.\n",
    "\n",
    "Then the image will be changed to another image, such as, an image of of a cafe latte. Then there will be speech generated and played to the user, \"Would you like another thing, such as croissant?\"\n",
    "\n",
    "The user can give an answer, \"No, but what is the wifi password?\" Then the image will be changed to another image, such as an image of a wifi router or a note displaying the wifi password. And so on. You get the idea.\n",
    "\n",
    "A user can use this app until they get bored. There is a button to quit the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1135a895-d5f7-4434-9baa-762c8ae901d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window\n",
    "# Display an image in the window\n",
    "# Play the audio file in the window\n",
    "# Create an interface to record speech or to upload the audio file\n",
    "# Create a way to regenerate the image and replay a different speech\n",
    "# Create a button to quit the app\n",
    "# Integrate the user interface with the core OpenAI API functions that you created previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a3331-7629-47ed-8c03-6df4d402bafd",
   "metadata": {},
   "source": [
    "## Evaluation and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacbdf3-7feb-4691-ab1a-48ae2aeae14d",
   "metadata": {},
   "source": [
    "After building and testing your multimodal AI app, consider the following questions:\n",
    "\n",
    "1. How does the integration of text, image, and audio enhance the language learning experience?\n",
    "2. What challenges did you face in designing the user interface for multi-modal interactions?\n",
    "3. How might you improve the app to make it more effective or user-friendly?\n",
    "\n",
    "Take some time to reflect on these questions and discuss your thoughts with your peers or instructor.\n",
    "\n",
    "Also, you can try to build the multimodal AI app outside Jupyter notebook. Put the app inside a Python script so you can run it in command line interface or terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3f388-9140-4ac9-9f61-a758c25dbf32",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e5002-fc78-4527-b89d-a7f2139bc5e9",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully built a multimodal AI application that integrates text, image, and audio processing. This language tutor app demonstrates the powerful potential of combining multiple AI technologies to create an immersive and interactive learning experience.\n",
    "\n",
    "Remember, this is just the beginning. There are many ways to expand and improve this app, such as implementing more sophisticated speech recognition, adding more scenarios, or incorporating user feedback to improve the AI's responses.\n",
    "\n",
    "Keep exploring and experimenting with multimodal AI applications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0455af7-c7a1-44be-9531-8d58835752d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

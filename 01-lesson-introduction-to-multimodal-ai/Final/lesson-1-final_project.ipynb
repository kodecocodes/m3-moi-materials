{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9387b5d3-5fa8-4f7d-b132-69836cb36103",
   "metadata": {},
   "source": [
    "# Lesson 1 Project: Introduction to Multimodal AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2680ef-279e-48ef-8656-aeb06f50b5ab",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211ef77-658d-4836-aea6-f5c39b1f0d59",
   "metadata": {},
   "source": [
    "Welcome to the first lesson on multimodal AI! While this course primarily focuses on images and speech, it also involves working with text. You should have already learned about text generation in previous courses and how to access the OpenAI text generation API. However, it never hurts to refresh your memory. By the end of this lesson, you will be able to:\n",
    "- Access the OpenAI text generation API\n",
    "- Ensure that the API responses are structured outputs\n",
    "\n",
    "These skills will serve as the foundation for learning the multimodal AI system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f6da9-561f-4f34-9ede-f1161eb3fb47",
   "metadata": {},
   "source": [
    "## Setting Up OpenAI Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4890826-33ca-4c67-9952-8545fb403a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (1.40.6)\n",
      "Requirement already satisfied: pydantic in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from pydantic) (2.20.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\skyko\\code\\projects\\kodeco\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install the libraries\n",
    "!pip install openai pydantic python-dotenv\n",
    "\n",
    "# Load the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up relevant environment variables\n",
    "# Make sure OPENAI_API_KEY=... exists in .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the OpenAI connection object\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aac8c6-c445-4f01-af26-0de83cbc7203",
   "metadata": {},
   "source": [
    "## Making an API Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05796a96-e70b-4735-8ec6-d469c278c9ba",
   "metadata": {},
   "source": [
    "To make a request to the OpenAI text generation API, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4de9e5-7168-443f-8a95-849181db4f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"The correct sentence should be: 'Alice eats an apple every day.'\", refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Make an API request\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an English grammar checker.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Check the grammar: 'Alice eat an apple every day.'\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03039f-7736-42c4-926f-22f9db014ae4",
   "metadata": {},
   "source": [
    "## Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eede58-e60d-4bfd-b803-12026cc3e7da",
   "metadata": {},
   "source": [
    "OpenAI introduced structured outputs, allowing you to enforce that the generated response from the API adheres to a JSON schema. This makes it easier to extract information without having to parse a raw string. To create an API request with structured outputs, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3e53df-0a30-4c16-bb22-fcb23a6dbdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParsedChatCompletionMessage[GrammarChecking](content='{\"wrong_sentence\":\"Alice eat an apple every day.\",\"correct_sentence\":\"Alice eats an apple every day.\",\"is_correct\":false}', refusal=None, role='assistant', function_call=None, tool_calls=[], parsed=GrammarChecking(wrong_sentence='Alice eat an apple every day.', correct_sentence='Alice eats an apple every day.', is_correct=False))\n"
     ]
    }
   ],
   "source": [
    "# Make an API request\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class GrammarChecking(BaseModel):\n",
    "    wrong_sentence: str\n",
    "    correct_sentence: str\n",
    "    is_correct: bool\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an English grammar checker.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Check the grammar: 'Alice eat an apple every day.'\"},\n",
    "    ],\n",
    "    response_format=GrammarChecking,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef41d50-496b-45b9-b00e-7bd99e2925a2",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096a17e-b465-4bb7-b6cc-4ee66f5cf8a0",
   "metadata": {},
   "source": [
    "In this lesson, you’ve refreshed your knowledge of the OpenAI text generation API. You’ve learned how to:\n",
    "- Set up the OpenAI client\n",
    "- Make an API request\n",
    "- Use structured outputs\n",
    "\n",
    "Although this multimodal AI course focuses on working with images and audio, you’ll also need to work with text. Text is an essential component of multimodal AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ed7de-973b-44d1-93a5-99b2d11a94f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
